% In this file you should put the actual content of the blueprint.
% It will be used both by the web and the print version.
% It should *not* include the \begin{document}
%
% If you want to split the blueprint content into several files then
% the current file can be a simple sequence of \input. Otherwise It
% can start with a \section or \chapter for instance.

\newcommand{\rew}{\tilde{r}^{\mathrm{h}}}
\newcommand{\why}[1]{\text{[#1]}}

\section{Probability Spaces and Expectation} \label{sec:prob-spac-expect}

In this section, we define the basic probability concepts on finite sets.
% Eventually, this section should be replaced by the Mathlib results using probability theory and measure theory.

\subsection{Definitions}

\begin{remark}
This document omits basic intuitive definitions, such as the comparison or arithmetic operations on random variables. Arithmetic operations on random variables are performed element-wise for each element of the sample set. Please see the Lean file for complete details.
\end{remark}

\begin{definition} \label{def:probability-measure}
A \emph{finite probability measure} $p\colon \Omega \to \Real_+$ on a finite set $\Omega$ is any function that satisfies
\[
\sum_{\omega \in \Omega } p(\omega) = 1. 
\]
\lean{Findist} \leanok
\end{definition}

\begin{definition}
The set of \emph{finite probability measures} $\Delta(\Omega)$ for a finite $\Omega$ is defined as
\[
\Delta(\Omega) := \left\{ p\colon \Omega \to \Real_+ \mid  \sum_{\omega \in \Omega } p(\omega) = 1 \right\}.
\]
\lean{Delta} \leanok
\end{definition}

\begin{definition} \label{def:probability-space}
A \emph{finite probability space} is $P = (\Omega, p)$, where $\Omega$ is a finite set referred to as the \emph{sample set}, $p\in \Delta(\Omega)$, and the $\sigma$-algebra is $2^{\Omega}$.
\lean{Finprob} \leanok
\end{definition}

\begin{definition}
A \emph{random variable} defined on a finite probability space $P$ is a mapping $\tilde{x}\colon \Omega \to \Real$.
\lean{Finrv} \leanok
\end{definition}

For the remainder of \cref{sec:prob-spac-expect}, we assume that $P = (\Omega, p)$ is a \emph{finite probability space}. All random variables are defined on the space $P$ unless specified otherwise.

\begin{definition}
A \emph{boolean} set is $\mathcal{B} = \left\{ \false, \true \right\}$.
\lean{Bool} \leanok
\end{definition}

\begin{definition} \label{def:expect}
The \emph{expectation} of a random variable $\tilde{x} \colon \Omega \to \Real$ is 
\[
\E \left[ \tilde{x} \right] := \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega).
\]
\lean{Finprob.expect} \leanok
\end{definition}

\begin{definition} \label{def:indicator}
An \emph{indicator} function $\I \colon \mathcal{B} \to \left\{ 0, 1 \right\}$ is defined for $b\in \mathcal{B}$ as
\[
\I(b) :=
\begin{cases}
1 &\text{if } b = \operatorname{true}, \\
0 &\text{if } b = \operatorname{false}.
\end{cases}
\]
\lean{Finprob.indicator} \leanok
\end{definition}

\begin{definition} \label{def:probability}
The \emph{probability} of $\tilde{b}\colon \Omega \to \mathcal{B}$ is defined as
\[
\PP\left[ \tilde{b} \right] := \E\left[\I(\tilde{b})\right].
\]
\lean{Finprob.probability}
\end{definition}

\begin{definition} \label{def:expect-cnd}
The \emph{conditional expectation} of $\tilde{x} \colon \Omega \to \Real$ conditioned on $\tilde{b} \colon \Omega \to \mathcal{B}$ is defined as
\[
\E \left[ \tilde{x} \mid  \tilde{b} \right] :=
\frac{1}{\PP[\tilde{b}]} \E \left[  \tilde{x} \cdot \I \circ \tilde{b} \right],
\]
where we define that $x / 0 = 0$ for each $x\in \Real$.
\lean{Finprob.expect_cnd} \leanok
\end{definition}

\begin{definition} \label{def:probability_cnd}
The \emph{conditional probability} of $\tilde{b}\colon \Omega \to \mathcal{B}$ on $\tilde{c}\colon \Omega \to \mathcal{B}$ is defined as
\[
\PP\left[ \tilde{b} \mid  \tilde{c} \right]  :=
\E \left[ \I(\tilde{b}) \mid  \tilde{c} \right].
\]
\end{definition}

\begin{remark}
It is common to prohibit conditioning on a zero probability event both for expectation and probabilities. In this document, we follow the Lean convention, where the division by $0$ is $0$; see \texttt{div\_zero}. However, even some basic probability and expectation results may require that we assume that the conditioned event does not have probability zero for it to hold. 
\end{remark}

\begin{definition} \label{def:expect-cnd-rv}
The \emph{random conditional expectation} of a random variable $\tilde{x} \colon \Omega \to \Real$ conditioned on $\tilde{y} \colon \Omega \to \mathcal{Y}$ for a finite set $\mathcal{Y}$ is the random variable $\E \left[ \tilde{x} \mid  \tilde{y} \right]\colon \Omega \to \Real$ is defined as
\[
\E \left[ \tilde{x} \mid  \tilde{y} \right](\omega)
:=
\E \left[ \tilde{x} \mid  \tilde{y} = \tilde{y}(\omega) \right], \quad \forall \omega \in \Omega.
\]
\lean{Finprob.expect_cnd_rv} \leanok
\end{definition}

\begin{remark}
The Lean file defines expectations more broadly for a data type $\rho$ which is more general than just $\Real$. The main reason to generalize to both $\Real$ and $\Real_+$. However, in principle, the definitions could be used to reason with expectations that go beyond real numbers and may include other algebras, such as vectors or matrices.
\end{remark}

\subsection{Basic Properties}

\begin{lemma} \label{lem:ind-and-eq-prod-ind}
Suppose that $\tilde{b}, \tilde{c} \colon \Omega \to \mathcal{B}$. Then:
\[
\I \left( \tilde{b} \wedge \tilde{c} \right) = \I ( \tilde{b} ) \cdot  \I ( \tilde{c} ),  
\]
where the equality applies for all $\omega \in \Omega$.
\end{lemma}

\begin{theorem} \label{thm:exp-zero-cond}
Suppose that $\tilde{c} \colon \Omega \to \mathcal{B}$ such that $\PP\left[ \tilde{c} \right] = 0$. Then for any $\tilde{x} \colon \Omega \to \Real$:
\[
\E \left[ \tilde{x} \mid \tilde{c} \right] = 0.
\]
\end{theorem}
\begin{proof}
Immediate from the definition and the fact that $0 \cdot  x = 0$ for $x\in \Real$.
\end{proof}

\begin{theorem} \label{thm:prob-zero-cond}
Suppose that $\tilde{c} \colon \Omega \to \mathcal{B}$ such that $\PP\left[ \tilde{c} \right] = 0$. Then for any $\tilde{b} \colon \Omega \to \Real$:
\[
\PP\left[ \tilde{b} \mid \tilde{c} \right] = 0.
\]
\end{theorem}
\begin{proof}
Immediate from \cref{thm:exp-zero-cond}.
\end{proof}

\begin{theorem} \label{thm:prob-eq-prob-cond-prob}
Suppose that $\tilde{b}, \tilde{c} \colon \Omega \to \mathcal{B}$, then
\[
\PP \left[ \tilde{b} \wedge \tilde{c} \right] 
=
\PP \left[ \tilde{b} \mid \tilde{c} \right] \cdot \PP \left[ \tilde{c} \right].
\]
\end{theorem}
\begin{proof} The property holds immediately when $\PP \left[ \tilde{c} \right] = 0$. Assume that $\PP \left[ \tilde{c} \right] > 0$. Then:
  \begin{align*}
    \PP \left[ \tilde{b} \wedge \tilde{c} \right]
    &= \E \left[ \I(\tilde{b} \wedge \tilde{c}) \right] && \why{\cref{def:probability}} \\
    &= \E \left[ \I(\tilde{b}) \cdot \I(\tilde{c}) \right] && \why{\cref{lem:ind-and-eq-prod-ind}}\\
    &= \frac{1}{\PP \left[ \tilde{c} \right]}\E \left[ \I(\tilde{b}) \cdot \I(\tilde{c}) \right] \cdot \PP \left[ \tilde{c} \right] && \cdot 1 \\
    &= \E \left[ \I(\tilde{b}) \mid \tilde{c} \right] \cdot \PP \left[ \tilde{c} \right]
    && \why{\cref{def:expect-cnd}} \\
    &= \PP \left[ \tilde{b} \mid \tilde{c} \right] \cdot \PP \left[ \tilde{c} \right] && \why{\cref{def:probability_cnd}}.
  \end{align*}
\end{proof}

\begin{lemma} \label{lem:prob-ge-measure}
  Let $\tilde{y}\colon \Omega \to \mathcal{Y}$ with a finite $\mathcal{Y}$. Then
  \[
   \PP[\tilde{y} = y(\omega)] \ge p(\omega ), \quad \omega \in \Omega . 
  \]
\end{lemma}
\begin{proof}
 \begin{align*}
   \PP[\tilde{y} = y(\omega)]
   &= \sum_{\omega' \in \Omega } p(\omega) \cdot \I(\tilde{y}(\omega') = \tilde{y}(\omega)) && \why{\cref{def:probability}} \\
   &\ge p(\omega ) && \omega \in \Omega \why{ and } p(\omega') \ge 0, \forall \omega '\in \Omega. 
 \end{align*}
\end{proof}


\begin{remark}
\cref{thm:exp-congr} shows the equivalence of expectations for surely equal random variables. 
\end{remark}

\begin{theorem} \label{thm:exp-add-rv}
  Random variables $\tilde{x}, \tilde{y} \colon \Omega \to \Real$ satisfy that
  \[
    \E \left[ \tilde{x} + \tilde{y} \right]
    =
    \E \left[ \tilde{x} \right] + \E \left[ \tilde{y} \right].
  \]
\end{theorem}
\begin{proof}
  From the distributive property of sums. 
\end{proof}

\begin{theorem} \label{thm:exp-const}
  A random variable $\tilde{x}\colon \Omega \to \Real$ and $c\in \Real $ satisfies that
  \[
   \E \left[ c \right] = c.
  \]
\end{theorem}


\begin{theorem} \label{thm:exp-add-const}
Suppose that $\tilde{x} \colon \Omega \to \Real$ and $c\in \Real$. Then
\[
\E \left[ c + \tilde{x} \right] = c + \E \left[ \tilde{x} \right].
\]
\lean{Finprob.exp_add_const}
\end{theorem}
\begin{proof}
  From \cref{thm:exp-add-rv,thm:exp-const}.
\end{proof}


\begin{theorem} \label{thm:exp-cnd-rv-add-const}
Suppose that $\tilde{x}, \tilde{y} \colon \Omega \to \Real$ and $\tilde{z}\colon \Omega \to \mathcal{V}$ are random variables and $c\in \Real$, such that $\tilde{y}(\omega) = c + \tilde{x}(\omega)$. Then
\[
\E \left[ \tilde{y} \mid \tilde{z} \right](\omega) = c + \E \left[ \tilde{x} \mid  \tilde{z} \right](\omega), \quad \forall \omega \in \Omega.
\]
\lean{Finprob.exp_cnd_rv_add_const}
\end{theorem}
\begin{proof}
From \cref{thm:exp-add-const}.
\end{proof}

\begin{theorem} \label{thm:exp-monotone}
  Suppose that $\tilde{x}, \tilde{y} \colon \Omega \to \Real$ satisfy that
  \[
    \forall \omega \in \Omega, p(\omega ) > 0 \Rightarrow \tilde{x}(\omega) \ge \tilde{y}(\omega).
  \]
  Then
  \[
   \E\left[\tilde{x}\right] \ge \E\left[\tilde{y}\right] .
  \]
 \lean{Finprob.exp_monotone}
\end{theorem}


\begin{theorem}[Congruence of Expectation] \label{thm:exp-congr}
  Suppose that $\tilde{x}, \tilde{z} \colon \Omega \to \Real$ satisfy that
  \[
     \forall \omega \in \Omega, p(\omega) > 0 \Rightarrow \tilde{x}(\omega) = \tilde{z}(\omega).
  \]
  Then
  \[
   \E \left[ \tilde{x} \right] = \E \left[ \tilde{z} \right].
  \]
\lean{Finprob.exp_congr}
\end{theorem}
\begin{proof}
Immediately from the congruence of sums.
\end{proof}
\subsection{The Laws of The Unconscious Statisticians}

\begin{theorem} \label{thm:exp-sum-val}
Let $\tilde{x} \colon \Omega \to \Real $ be a random variable. Then:
\[
\E \left[ \tilde{x} \right]
=
\sum_{x\in \tilde{x}(\Omega)} \PP \left[ \tilde{x} = x \right] \cdot  x. 
\]
\lean{Finprob.exp_sum_val}
\end{theorem}
\begin{proof}
Let $\mathcal{X} := \tilde{x}(\Omega)$, which is a finite set. Then:
\begin{align*}
\E \left[ \tilde{x} \right]
&= \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega) && \why{\cref{def:expect}} \\
&= \sum_{\omega \in \Omega } \sum_{x\in \mathcal{X}} p(\omega ) \cdot \tilde{x}(\omega) \cdot \I(x =  \tilde{x}(\omega)) && \why{??} \\
&= \sum_{\omega \in \Omega } \sum_{x\in \mathcal{X}} p(\omega ) \cdot x \cdot \I(x =  \tilde{x}(\omega)) && \why{??} \\
&= \sum_{x\in \mathcal{X}} x \cdot \sum_{\omega \in \Omega }  p(\omega ) \cdot  \I(x =  \tilde{x}(\omega))  && \why{??}\\
&= \sum_{x\in \mathcal{X}} x \cdot \E \left[  \I(x =  \tilde{x}(\omega)) \right] && \why{\cref{def:expect}} \\
&= \sum_{x\in \mathcal{X}} x \cdot \PP \left[x =  \tilde{x}(\omega) \right]. && \why{\cref{def:probability}} 
\end{align*}
\end{proof}

The following theorem generalizes the theorem above. 
\begin{theorem}\label{thm:exp-sum-val-cnd}
Let $\tilde{x} \colon \Omega \to \Real$ and $\tilde{b} \colon \Omega \to \mathcal{Y}$ be random variables. Then:
\[
  \E \left[ \tilde{x} \mid \tilde{b} \right] 
  =
  \sum_{x\in \tilde{x}(\Omega)} \PP \left[ \tilde{x} = x \mid \tilde{b}  \right] \cdot x.
\]
\lean{Finprob.exp_sum_val_cnd}
\end{theorem}

\begin{theorem} \label{thm:exp-sum-val-cnd-rv}
Let $\tilde{x} \colon \Omega \to \Real$ and $\tilde{y} \colon \Omega \to \mathcal{Y}$ be random variables with $\mathcal{Y}$ finite. Then:
\[
  \E \left[ \E \left[ \tilde{x} \mid  \tilde{y} \right] \right]
  =
  \sum_{y\in \mathcal{Y}} \E \left[ \tilde{x} \mid  \tilde{y} = y \right] \cdot
                       \PP\left[ \tilde{y} = y \right].
\]
\lean{Finprob.exp_sum_val_cnd_rv}
\end{theorem}


\subsection{Total Expectation and Probability}


\begin{theorem}[Law of Total Probability] \label{thm:total-probability}
Let $\tilde{b} \colon \Omega \to \mathcal{B}$ and $\tilde{y} \colon \Omega \to \mathcal{Y}$ be random variables with a finite set $\mathcal{Y}$. Then:
\[
\sum_{y\in \mathcal{Y}} \PP\left[\tilde{b} \wedge (\tilde{y} = y) \right] = \PP \left[ \tilde{b} \right].
\]
\lean{Finprob.total_expectation}
\end{theorem}


\begin{theorem}[Law of Total Expectation] \label{thm:total-expectation}
Let $\tilde{x} \colon \Omega \to \mathcal{X}$ and $\tilde{y} \colon \Omega \to \mathcal{Y}$ be random variables with a finite set $\mathcal{Y}$. Then:
\[
\E\left[\E\left[\tilde{x} \mid  \tilde{y}\right]\right] = \E \left[ \tilde{x} \right].
\]
\lean{Finprob.total_expectation}
\end{theorem}
\begin{proof}
Recall that we are allowing the division by 0 and assume that $x / 0 = 0$.
\begin{align*} 
  \E\left[\E\left[\tilde{x} \mid  \tilde{y}\right]\right]
  &= \sum_{\omega \in \Omega }  p (\omega) \cdot  \E \left[ \tilde{x} \mid  \tilde{y} \right](\omega) && \why{\cref{def:expect}}\\
  &= \sum_{\omega \in \Omega }  p (\omega) \cdot  \E \left[ \tilde{x} \mid  \tilde{y}  = \tilde{y}(\omega) \right] && \why{\cref{def:expect-cnd-rv}}\\
  &= \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y}= \tilde{y}(\omega)\right]} \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega') \cdot  \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right)   && \why{\cref{def:expect-cnd}} \\
  &=  \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega') \cdot \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y}= \tilde{y}(\omega)\right]} \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right)  && \why{rearrange} \\
  &=  \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega') \cdot \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y} = \tilde{y}(\omega')\right]} \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right)  && \why{equals when } \tilde{y}(\omega') = \tilde{y}(\omega) \\
  &=  \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega')    && \why{see below}  \\
  &= \E \left[ \tilde{x} \right].
\end{align*}
Above, we used the fact that
\[
  p(\omega') \cdot \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y} = \tilde{y}(\omega')\right]} \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right) =  p(\omega'),  
\]
which follows by analyzing two cases. First, when $p(\omega') = 0$, then the equality holds immediately. If $p(\omega') > 0$ then by \cref{lem:prob-ge-measure}, $\PP \left[ \tilde{y} = \tilde{y}(\omega')\right] > 0$, we get from \cref{def:probability} that
\[
  \sum_{\omega \in \Omega } \frac{p(\omega)}{\PP \left[ \tilde{y} = \tilde{y}(\omega')\right]} \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right) 
    =
    \frac{\PP \left[ \tilde{y} = \tilde{y}(\omega')\right]}{\PP \left[ \tilde{y} = \tilde{y}(\omega')\right]}
    = 1,
  \]
  which completes the step.
\end{proof}
% The following proof is simpler but may require some more advanced properties.
% \begin{proof}[Alternate proof]
% \begin{align*}
% \E\left[\E\left[\tilde{x} \mid  \tilde{y}\right] \right]
% &= \sum_{y\in \mathcal{Y}} \E\left[\tilde{x} \mid \tilde{y} = y \right] \cdot \PP\left[ \tilde{y} = y \right]  \\
% &= \sum_{y\in \mathcal{Y}} \sum_{x\in \mathcal{X}} x \cdot \PP\left[\tilde{x} = x \mid \tilde{y} = y \right] \cdot \PP\left[ \tilde{y} = y \right]  \\
% &= \sum_{y\in \mathcal{Y}} \sum_{x\in \mathcal{X}} x \cdot  \PP\left[\tilde{x} = x, \tilde{y} = y \right]   \\
% &=  \sum_{x\in \mathcal{X}} x \cdot \sum_{y\in \mathcal{Y}} \PP\left[\tilde{x} = x, \tilde{y} = y \right]   \\
% &=  \sum_{x\in \mathcal{X}} x \cdot  \PP\left[\tilde{x} = x \right]  = \E \left[ \tilde{x} \right].
% \end{align*}
% \end{proof}

\section{Formal Decision Framework}

\subsection{Markov Decision Process}

\begin{definition} \label{def:MDP}
A \emph{Markov decision process} $M := (\mathcal{S}, \mathcal{A}, P, r)$ consists of a finite nonempty set of states $\mathcal{S}$, a finite nonempty set of actions $\mathcal{A}$, transition function $P\colon \mathcal{S} \times \mathcal{A} \to  \Delta(\mathcal{S})$, and a reward function $r \colon \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \Real$.
\lean{MDPs.MDP} \leanok
\end{definition}

\subsection{Histories}

We implicitly assume in the remainder of the section an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$.
\begin{definition} \label{def:Hist}
A \emph{history} $h$ in a set of histories $\mathcal{H}$ is a sequence of states and actions defined for $M$ recursively as
\[
  h := \langle s \rangle, \quad
  \why{or} \quad
  h := \langle h', s, a \rangle,
\]
where $s \in \mathcal{S}$, $a\in \mathcal{A}$, and $h'\in \mathcal{H}$.
\lean{MDPs.Hist} \leanok
\end{definition}

\begin{definition} \label{def:hist-length}
The \emph{length} $l\colon \mathcal{H} \to \Nats$ of a history $h$ is defined as
\begin{align*}
l(\langle s \rangle) &:= 0, \\
l(\langle h', s, a \rangle) &:= 1 + h', \qquad h' \in \mathcal{H}. 
\end{align*}
\end{definition}

\begin{definition} \label{def:hist-ne}
The set $\mathcal{H}_{\mathrm{NE}}$ of \emph{non-empty histories} is
\[
\mathcal{H}_{\mathrm{NE}} := \left\{ h \in \mathcal{H} \mid  l(h) \ge 1 \right\}.
\]
\lean{MDPs.HistNE} \leanok
\end{definition}

\begin{definition}\label{def:histories}
  \emph{Following histories} $\mathcal{H}(h,t) \subseteq \mathcal{H}$ for $h\in \mathcal{H}$ of length $t\in \Nats$ are defined recursively as
  \[
    \mathcal{H}(h,t) :=
    \begin{cases}
    \left\{  h \right\}  &\text{if } t = 0, \\
    \left\{  \langle h', a, s \rangle \mid h\in \mathcal{H}(h', t-1), a\in \mathcal{A}, s\in \mathcal{S}  \right\}  &\text{otherwise}. \\
    \end{cases}
  \]
\end{definition}

\begin{definition} \label{def:histories-horizon}
  The set of \emph{histories} $\mathcal{H}_{t}$ of \emph{length} $t \in \Nats$ is defined recursively as
  \[
    \mathcal{H}_t =
    \begin{cases}
      \left\{ \langle s \rangle \mid  s\in \mathcal{S} \right\} &\text{if } t = 0, \\
      \left\{ \langle h, a, s\rangle \mid h \in \mathcal{H}_{t-1}, a\in \mathcal{A}, s\in \mathcal{S} \right\} &text{otherwise}.
    \end{cases}
  \]
  \lean{MDPs.HistoriesHorizon} \leanok
\end{definition}

\begin{theorem} \label{thm:hist-lenth-eq-horizon}
  For $h \in \mathcal{H}$:
  \[
   |h'| = |h| + t, \qquad \forall h'\in \mathcal{H}(h, t).
  \]
\lean{MDPs.hist_lenth_eq_horizon} 
\end{theorem}
\begin{proof}
The theorem follows by induction on $t$ from the definition.  
\end{proof}

\begin{definition}
  We use $\tilde{s}_k\colon \mathcal{H} \to \mathcal{S}$ to denote the 0-based $k$-th state of each history.
  \lean{MDPs.state}\leanok
\end{definition}

\begin{definition}
  We use $\tilde{a}_k\colon \mathcal{H} \to \mathcal{A}$ to denote the 0-based $k$-th action of each history.
  \lean{MDPs.action}\leanok
\end{definition}

\begin{definition} \label{def:reward}
The \emph{history-reward} random variable $\rew\colon \mathcal{H} \to \Real$ for $h = \langle h', a, s \rangle \in  \mathcal{H}$ for $h'\in \mathcal{H}$, $a\in \mathcal{A}$, and $s\in \mathcal{S}$ is defined recursively as
\[
\rew(h) := r(s_{\mathrm{l}}(h'), a, s) + r_{\mathrm{h}}(h').
\]
\lean{MDPs.reward} \leanok
\end{definition}

\begin{definition}\label{reward_at}
The \emph{history-reward} random variable $\rew_k \colon \mathcal{H} \to \Real$ for $h = \langle h', a, s \rangle \in  \mathcal{H}$ for $h'\in \mathcal{H}$, $a\in \mathcal{A}$, and $s\in \mathcal{S}$ is defined as the $k$-th reward (0-based) of a history.
\lean{MDPs.reward_at} \leanok
\end{definition}

\begin{definition}\label{reward_to}
The \emph{history-reward} random variable $\rew_{\le k} \colon \mathcal{H} \to \Real$ for $h = \langle h', a, s \rangle \in  \mathcal{H}$ for $h'\in \mathcal{H}$, $a\in \mathcal{A}$, and $s\in \mathcal{S}$ is defined as the sum of all $k$-th or earlier rewards (0-based) of a history.
\lean{MDPs.reward_to} \leanok
\end{definition}

\begin{definition}\label{reward_from}
The \emph{history-reward} random variable $\rew_{\ge k}\colon \mathcal{H} \to \Real$ for $h = \langle h', a, s \rangle \in  \mathcal{H}$ for $h'\in \mathcal{H}$, $a\in \mathcal{A}$, and $s\in \mathcal{S}$ is defined as the sum of $k$-th or later reward (0-based) of a history.
\lean{MDPs.reward_from} \leanok
\end{definition}

\subsection{Policies}

\begin{definition} \label{def:decision-rule}
The set of \emph{decision rules} $\mathcal{D}$ is defined as \(\mathcal{D} := \mathcal{A}^{\mathcal{S}}. \) A single action $a \in \mathcal{A}$ can also be interpreted as a decision rule $\mathcal{d} := s \mapsto a$.
\lean{MDPs.DecisionRule}\leanok
\end{definition}

\begin{definition} \label{def:policy-hr}
The set of \emph{history-dependent policies} is \(\PiHR :=  \Delta(\mathcal{A})^{\mathcal{H}}. \)
\lean{MDPs.PolicyHR} \leanok
\end{definition}


\begin{definition} \label{def:policy-md}
The set of \emph{Markov deterministic policies} $\PiMD$ is \(\PiMD :=  \mathcal{D}^{\Nats}. \)
A Markov deterministic policy $\pi \in \PiMD$ can also be interpreted as $\bar{\pi} \in \PiHR$:
\[
  \bar{\pi}(h) := \delta \left[  \pi(l(h), s_{\mathrm{l}}(h)) \right],
\]
where $\delta$ is the Dirac distribution, $l$ is the length defined in \cref{def:hist-length}, and $s_{\mathrm{l}}$ is the history's last state.
\lean{MDPs.PolicyMD}\leanok
\end{definition}

\begin{definition} \label{def:policy-sd}
The set of \emph{stationary deterministic policies} $\Pi_{\mathrm{SD}}$ is defined as \(\Pi_{\mathrm{SD}} := \mathcal{D}. \)
A stationary policy $\pi \in \Pi_{\mathrm{SD}}$ can be interpreted as $\bar{\pi} \in \PiHR$:
\[
  \bar{\pi}(h) := \delta \left[  \pi(s_{\mathrm{l}}(h)) \right],
\]
where $\delta$ is the Dirac distribution and $s_{\mathrm{l}}$ is the history's last state.
\lean{MDPs.PolicySD}\leanok
\end{definition}

\subsection{Distribution}


\begin{definition}\label{def:hist-dist}
The \emph{history probability distribution} $p^{\mathrm{h}}_T \colon  \PiHR \to \Delta(\mathcal{H}(h,t))$  and $\pi \in \PiHR$ is defined for each $T \in \Nats$ and $h\in \mathcal{H}(\hat{h},t)$ as
\[
(p^{\mathrm{h}}_T(\pi))(h) :=
\begin{cases}
\I(h = \hat{h}) &\text{if } T = 0, \\
p_{T-1}^{\mathrm{h}}(h', \pi) \cdot \pi(h',a) \cdot  p(s_{\mathrm{l}}(h'), a , s) &\text{if } T > 1 \wedge h = \langle h', a, s \rangle.
\end{cases}
\]
Moreover, the function $p^{\mathrm{h}}$ maps policies to correct probability distribution.
\lean{MDPs.HistDist} \leanok
\end{definition}

\begin{definition}\label{def:expect-h}
The \emph{history-dependent expectation} is defined for each $t\in \Nats$, $\pi\in \PiHR$, $\hat{h}\in \mathcal{H}$ and a $\tilde{x}\colon \mathcal{H} \to \Real$ as
\begin{equation*}
\E^{\hat{h}, \pi, t} [\tilde{x}]
:= \E \left[ \tilde{x} \right] 
= \sum_{h \in \mathcal{H}(\hat{h}, t)} p^{\mathrm{h}}(h, \pi) \cdot \tilde{x}(h).
\end{equation*}
In the $\E$ operator above, the random variable $\tilde{x}$ lives in a probability space $(\Omega, p)$ where $\Omega = \mathcal{H}(\hat{h}, t)$ and $p(h) = p^{\mathrm{h}}(h, \pi), \forall h\in \Omega$.
Moreover, if $\hat{h}$ is a state, then it is interpreted as a history with the single initial state.
\lean{MDPs.expect_h} \leanok
\end{definition}


\begin{definition}\label{def:expect-h-cnd}
The \emph{history-dependent expectation} is defined for each $t\in \Nats$, $\pi\in \PiHR$, $\hat{h}\in \mathcal{H}$, $\tilde{x}\colon \mathcal{H} \to \Real$, $\tilde{b}\colon \mathcal{H} \to \mathcal{\mathcal{B}}$ as
\begin{equation*}
\E^{\hat{h}, \pi, t} [\tilde{x} \mid \tilde{b} ]
:= \E \left[ \tilde{x} \mid  \tilde{b} \right].
\end{equation*}
In the $\E$ operator above, the random variables $\tilde{x}$ and $\tilde{b}$ live in a probability space $(\Omega, p)$ where $\Omega = \mathcal{H}(\hat{h}, t)$ and $p(h) = p^{\mathrm{h}}(h, \pi), \forall h\in \Omega$.
Moreover, if $\hat{h}$ is a state, then it is interpreted as a history with the single initial state.
\lean{MDPs.expect_h_cnd} \leanok
\end{definition}

\begin{definition}\label{def:expect-h-cnd-rv}
The \emph{history-dependent expectation} is defined for each $t\in \Nats$, $\pi\in \PiHR$, $\hat{h}\in \mathcal{H}$, $\tilde{x}\colon \mathcal{H} \to \Real$, $\tilde{y}\colon \mathcal{H} \to \mathcal{\mathcal{V}}$ as
\begin{equation*}
\E^{\hat{h}, \pi, t} [\tilde{x} \mid \tilde{y} ](h)
:= \E \left[ \tilde{x} \mid \tilde{y} = \tilde{y}(h) \right](h), \quad\forall h\in \mathcal{H}(\hat{h}, t).
\end{equation*}
In the $\E$ operator above, the random variables $\tilde{x}$ and $\tilde{h}$ live in a probability space $(\Omega, p)$ where $\Omega = \mathcal{H}(\hat{h}, t)$ and $p(h) = p^{\mathrm{h}}(h, \pi), \forall h\in \Omega$.
Moreover, if $\hat{h}$ is a state, then it is interpreted as a history with the single initial state.
\lean{MDPs.expect_h_cnd_rv} \leanok
\end{definition}


\subsection{Basic Properties}


\begin{theorem} \label{thm:exph-congr}
Assume $\tilde{x} \colon \mathcal{H} \to \Real$ and $c \in \Real$. Then $\forall h\in \mathcal{H}, \pi\in \PiHR, t \in \Nats$:
\[
  \E^{\hat{h}, \pi, t} \left[ c + \tilde{x} \right]
  =
  c + \E^{\hat{h}, \pi, t} \left[ \tilde{x} \right].
\]
\lean{MDPs.exph_congr} \leanok
\end{theorem}
\begin{proof}
Directly from \cref{thm:exp-congr}. 
\end{proof}

\begin{theorem} \label{thm:exph-add-rv}
Suppose that $\tilde{x}, \tilde{y} \colon \mathcal{H} \to \Real $. Then $\forall h\in \mathcal{H}, \pi\in \PiHR, t \in \Nats$:
\[
  \E^{\hat{h}, \pi, t} \left[ \tilde{x} + \tilde{y} \right]
  =
  \E^{\hat{h}, \pi, t} \left[ \tilde{x} \right] + \E^{\hat{h}, \pi, t} \left[ \tilde{y} \right].
\]
\lean{MDPs.exph_add_rv}
\end{theorem}
\begin{proof}
  From \cref{thm:exp-add-rv}.
\end{proof}

\begin{theorem} \label{thm:exph-const}
Suppose that $c\in \Real$. Then $\forall h\in \mathcal{H}, \pi\in \PiHR, t \in \Nats$:
\[
  \E^{\hat{h}, \pi, t} \left[ c \right] = c.
\]
\lean{MDPs.exph_const}
\end{theorem}
\begin{proof}
  From \cref{thm:exp-const}.
\end{proof}

\begin{theorem} \label{thm:exph-add-const}
Suppose that $\tilde{x}, \tilde{y} \colon \mathcal{H} \to \Real $ satisfy that $\tilde{x}(h) = \tilde{y}(h), \forall h \in \mathcal{H}$. Then $\forall h\in \mathcal{H}, \pi\in \PiHR, t \in \Nats$:
\[
  \E^{\hat{h}, \pi, t} \left[ \tilde{x} \right]
  =
  c + \E^{\hat{h}, \pi, t} \left[ \tilde{y} \right].
\]
\lean{MDPs.exph_add_const} 
\end{theorem}
\begin{proof}
From \cref{thm:exp-add-const}.
\end{proof}

\begin{theorem}\label{thm:expret-eq-sum-rew}
For each $\hat{h}\in \mathcal{H}$, $\pi \in \PiHR$, and $t\in \Nats$:
\[
\E^{\hat{h}, \pi, t} \left[ \rew \right]
=
\E^{\hat{h}, \pi, t} \left[ \sum_{k=0}^{|\tilde{\id}|-1}  r(\tilde{s}_k, \tilde{a}_k, \tilde{s}_{k+1}) \right],
\]
where $\tilde{\id}(h)$ is the identity function, $|\cdot|$ is the length of a history (0-based), $\tilde{s}_k\colon \mathcal{H} \to \mathcal{S}$ and $\tilde{a}_k\colon \mathcal{H} \to \mathcal{A}$ are the 0-based $k$-th state and action, respectively of each history.
\lean{MDPs.expret_eq_sum_rew} \leanok
\end{theorem}
\begin{proof}
Follows from \cref{thm:exph-congr} and the equality of the reward function $\tilde{r}^{\mathrm{h}}$ and the sum in the expectation.
\end{proof}

\begin{theorem}\label{thm:sum-rew-eq-sum-rew-rg}
For each $h\in \mathcal{H}$, $\pi \in \PiHR$, and $t\in \Nats$:
\[
\E^{h, \pi, t} \left[  \rew \right]
=
\rew(h) + \E^{h, \pi, t} \left[ \rew_{\ge k_0} \right],
\]
where $k_0:=|h|$.
  \lean{MDPs.sum_rew_eq_sum_rew_rg }
\end{theorem}
\begin{proof}
Follows from \cref{thm:exph-add-const}.
\end{proof}

\begin{theorem}\label{thm:sum-rew-cnd}
For each $\hat{h}\in \mathcal{H}$, $\pi \in \PiHR$, $t\in \Nats$, $h\in \mathcal{H}$:
\begin{equation*}
  \PP^{\hat{h},\pi.t}[\tilde{s}_{k_0}=\tilde{s}_{k_0}(\omega) \wedge \tilde{a}_{k_0} = \tilde{a}_{k_0}(\omega) ] > 0
\quad\Rightarrow \quad
  \E^{\hat{h}, \pi, t} \left[ \rew_{k_0} \mid  \tilde{s}_{k_0}, \tilde{a}_{k_0}\right](h) = \rew_{k_0}(h), \forall h \in \mathcal{H}.
\end{equation*}
where $k_0:= |\hat{h}|$.
\lean{MDPs.sum_rew_eq_sum_rew_rg }
\end{theorem}
\begin{proof}
 From \cref{thm:exp-const}. 
\end{proof}

\begin{theorem} \label{thm:exph-horizon-trim}
Assume $h \in \mathcal{H}$ and $f \colon \mathcal{H} \to \Real$ such that $s_0 := s_{\mathrm{l}}(h)$
\[
f(\left< h, a, s \right>) = f(\left< s_0, a, a \right>) , \forall a\in \mathcal{A}, s\in \mathcal{S}.
\]
Then
\[
\E^{h,\pi,1}\left[ \tilde{f} \right] =   
\E^{s_0,\pi,1}\left[ \tilde{f} \right].
\]
\end{theorem}
\lean{MDPs.exph_horizon_trim }
\begin{proof}
  Directly from the definition of the expectation.
\end{proof}

\subsection{Total Expectation}

\begin{theorem}[Total Expectation] \label{thm:total-expectation-h}
  For each $h\in \mathcal{H}$, $\pi\in \PiHR$, $t\in \Nats$, $\tilde{x}\colon \mathcal{H} \to \Real$ and $\tilde{y}\colon \mathcal{H} \to \mathcal{V}$:
  \[
    \E^{h, \pi, t} \left[ \E^{h, \pi, t} \left[  \tilde{x} \mid  \tilde{y} \right] \right]
    =
    \E^{h, \pi, t} \left[ \tilde{x} \right].
  \]
\lean{MDPs.total_expectation_h}
\end{theorem}
\begin{proof}
From \cref{thm:total-expectation}.
\end{proof}

\begin{theorem}\label{thm:exp-horizon-cut}
Suppose that the random variable $\tilde{x}\colon \mathcal{H} \to \Real$ satisfies for some $k, t \in \Nats$, with $k \le t$, that
  \[
   \tilde{x}(h) = \tilde{x}(h_{\le k}) , \forall h\in  \mathcal{H},
 \]
 where $h_{\le k}$ is the prefix of $h$ of length $k$. Then for each $h\in \mathcal{H}, \pi\in \PiHR$:
 \[
  \E^{h,\pi,t} \left[ \tilde{x} \right]  = \E^{h,\pi,k} \left[  \tilde{x} \right].
 \]
 %\lean{MDPs.exp_horizon_cut}
\end{theorem}


\subsection{Conditional Properties}

\begin{theorem} \label{thm:exph-cond-eq-hist}
For each $h\in \mathcal{H}$, $k_0 := |h|$, $\pi\in \PiHR$, $t\in \Nats$, $\tilde{x}\colon \mathcal{H} \to \Real$, $s\in \mathcal{S}$, $a\in \mathcal{A}$:
\[
  \E^{h,\pi,t+1}[\tilde{x} \mid \tilde{a}_{k_0} = a, \tilde{s}_{k_0+1} = s]
  =
  \E^{\langle h, a, s\rangle,\pi,t}[\tilde{x}].
\]
\lean{MDPs.exph_cond_eq_hist}
\end{theorem}
\begin{proof}
This should follow by algebraic manipulation.
\end{proof}

\section{Dynamic Program: History-Dependent Finite Horizon}

In this section, we derive dynamic programming equations for histories. We assume an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$ throughout this section.

The main idea of the proof is to:
\begin{enumerate}
\item Derive (exponential-size) dynamic programming equations for the history-dependent value function of history-dependent policies
  \begin{enumerate}
    \item Define the value function
    \item Define an optimal value function
  \end{enumerate}
\item Show that value functions decompose to equivalence classes
\item Show that the value function for the equivalence classes can be computed efficiently
\end{enumerate}

\subsection{Definitions}

\begin{definition} \label{def:ObjectiveFH}
  A finite horizon objective definition is given by $O := (s_0, T)$ where $s_0\in \mathcal{S}$ is the initial state and $T\in \Nats$ is the horizon.
  \lean{MDPs.ObjectiveFH} \leanok
\end{definition}

In the reminder of the section, we assume an objective $O = (s_0, T)$.

\begin{definition} \label{sec:objective-fh}
  The \emph{finite horizon objective function} for and objective $O$ is $\pi\in \PiHR$ is defined as
  \[
    \rho(\pi, O)
    :=
    \E^{s_0, \pi, T} \left[\rew \right].
  \]
  \lean{MDPs.objective_fh} \leanok
\end{definition}

\begin{definition}\label{def:optimal-fh}
  A policy $\pi\opt\in \PiHR$ is \emph{return optimal} for an objective $O$ if
  \[
   \rho(\pi\opt, O) \ge \rho(\pi, O), \quad \forall \pi \in \PiHR.
 \]
 \lean{MDPs.OptimalVF_fh} \leanok
\end{definition}

\begin{definition} \label{def:values-h}
  The \emph{set of history-dependent value functions} $\mathcal{U}$ is defined as
  \[
   \mathcal{U} := \Real^{\mathcal{H}}.
 \]
 \lean{MDPs.ValuesH} \leanok
\end{definition}

\begin{definition} \label{def:u-pi}
A \emph{history-dependent policy value function} $\hat{u}^{\pi}_t\colon \mathcal{H} \to \Real$ for each $h\in \mathcal{H}$, $\pi \in \PiHR$, and $t\in \Nats$ is defined as
  \[
   \hat{u}^{\pi}_t(h) := \E^{h, \pi, t} \left[ \rew_{\ge |h|} \right],
 \]
 \lean{MDPs.hvalue_π} \leanok
\end{definition}


\begin{definition}
The optimal history-dependent value function $\hat{u}_t\opt\colon \mathcal{H} \to \Real$ is defined for a horizon $t\in \Nats$ as
 \[
  \hat{u}\opt_t(h) := \sup_{\pi\in \PiHR} \hat{u}_t^{\pi}(h). 
 \] 
\end{definition}

The following definition is another way of defining an optimal policy.
\begin{definition} \label{def:optimal-fh}
For each $t\in \Nats$, a policy $\pi\opt \in \PiHR$ is optimal if
\[
   \hat{u}_t^{\pi\opt}(h) \ge \hat{u}_t^{\pi}(h), \quad \forall \pi \in \PiHR, h\in \mathcal{H}.
 \]
 \lean{MDPs.Optimal_fh}\leanok
\end{definition}

\begin{theorem} \label{def:optimalvf-imp-optimal}
A policy $\pi\opt \in \PiHR$ optimal in \cref{def:optimal-fh} is also optimal in \cref{def:optimal-fh} for any initial state $s_0$ and horizon $T$.
\lean{MDPs.optimalvf_imp_optimal}
\end{theorem}

\subsection{History-dependent Dynamic Program}

The following definitions of history-dependent value functions use a dynamic program formulation.

\begin{definition} \label{def:DPhpi}
The \emph{history-dependent policy Bellman operator} $L_{\mathrm{h}}^{\pi} \colon \mathcal{U} \to \mathcal{U}$ is defined for each $\pi\in \PiHR$ as
  \[
    (L_{\mathrm{h}}^{\pi} \tilde{u}) (h)
    :=
    \E^{h, \pi, 1} \left[ \rew_{|h|} + \tilde{u} \right], \quad \forall h\in \mathcal{H}, \tilde{u} \in \mathcal{U},
  \]
  where the value function $\tilde{u}$ is interpreted as a random variable on defined on the sample space $\Omega = \mathcal{H}$.
  \lean{MDPs.DPhπ} \leanok
\end{definition}

\begin{definition}\label{def:DPhopt} 
The \emph{history-dependent optimal Bellman operator} $L_{\mathrm{h}}\opt \colon \mathcal{U} \to \mathcal{U}$ is defined as
  \[
    (L\opt_{\mathrm{h}} \tilde{u}) (h)
    :=
    \max_{a\in \mathcal{A}} \E^{h, a, 1} \left[ \rew_{|h|} + \tilde{u} \right], \quad \forall h\in \mathcal{H}, \tilde{u} \in \mathcal{U},
  \]
  where the value function $\tilde{u}$ is interpreted as a random variable on defined on the sample space $\Omega = \mathcal{H}$.
  \lean{MDPs.DPhopt}\leanok
\end{definition}

\begin{definition} \label{def:u-dp-pi}
  The history-dependent \emph{DP value function} $u_t^{\pi} \in \mathcal{U}$ for a policy $\pi\in \PiHR$ and $t\in \Nats$ is defined as
  \[
   u_t^{\pi} :=
   \begin{cases}
     0 &\text{if } t = 0, \\
     L_{\mathrm{h}}^{\pi} u_{t-1}^{\pi} &\text{otherwise}.
   \end{cases}
 \]
 \lean{MDPs.u_dp_π}\leanok
\end{definition}

\begin{definition} \label{def:u-dp-opt}
  The history-dependent \emph{DP value function} $u_t\opt \in \mathcal{U}$ for $t\in \Nats$ is defined as
  \[
   u_t\opt  :=
   \begin{cases}
     0 &\text{if } t = 0, \\
     L_{\mathrm{h}}\opt u_{t-1}\opt &\text{otherwise}.
   \end{cases}
 \]
 \lean{MDPs.u_dp_opt}\leanok
\end{definition}

\begin{lemma}\label{thm:dp-opt-ge-dp-pi}
  Suppose that $u^1, u^2 \in \mathcal{U}$ satisfy that $u^1(h) \ge u^2(h), \forall h\in \mathcal{H}$. Then
  \[
   (L_{\mathrm{h}}\opt u^1)(h) \ge  
   (L_{\mathrm{h}}^{\pi} u^2)(h), \quad \forall \pi\in \PiHR, h\in \mathcal{H}.
  \]
\end{lemma}
\lean{MDPs.dp_opt_ge_dp_pi}
\begin{proof}
  From \cref{thm:exp-monotone}.
\end{proof}

The following theorem shows the history-dependent value function can be computed by the dynamic program. The following theorem is akin to \citep[theorem~4.2.1]{Puterman2005}.
\begin{theorem} \label{thm:dph-correct-vf}
For each $\pi\in \PiHR$ and $t\in \Nats$:
  \[
    \hat{u}^{\pi}_t(h)
    =
    u^{\pi}_t(h), \quad \forall h\in \mathcal{H}.
  \]
  \lean{MDPs.dph_correct_vf }
\end{theorem}
\begin{proof}
By induction on $t$. The base case for $t=0$ follows from the definition. The inductive case for $t + 1$ follows for each $h\in \mathcal{H}$ when $|h| = k_0$ as
{\allowdisplaybreaks
\begin{align*}
\hat{u}_{t+1}^{\pi}(h)
&= \E^{h, \pi, t+1} \left[ \rew_{\ge k_0}  \right] && \why{\cref{def:u-pi}}\\
&= \E^{h, \pi, t+1} \left[  \E^{h, \pi, t+1} \left[ \rew_{\ge k_0} \mid \tilde{a}_{k_0}, \tilde{s}_{k_0+1} \right] \right]  && \why{\cref{thm:total-expectation-h}} \\
&= \E^{h, \pi, t+1} \left[\rew_{k_0} +  \E^{h, \pi, t+1} \left[  \rew_{\ge k_0+1} \mid \tilde{a}_{k_0}, \tilde{s}_{k_0+1} \right] \right] && \why{\cref{thm:sum-rew-cnd}} \\
&= \E^{h, \pi, t+1} \left[\rew_{k_0} +  \E^{\langle h,\tilde{a}_{k_0},\tilde{s}_{k_0+1}\rangle, \pi, t} \left[ \rew_{\ge k_0+1} \right]\right] && \why{\cref{thm:exph-cond-eq-hist}} \\
&= \E^{h, \pi, t+1} \left[\rew_{k_0} +  \hat{u}_t(\langle h,\tilde{a}_{k_0},\tilde{s}_{k_0+1}\rangle; \pi) \right] && \why{\cref{def:u-pi}} \\
&= \E^{h, \pi, t+1} \left[\rew_{k_0} +  u^{\pi}_t(\langle h,\tilde{a}_{k_0},\tilde{s}_{k_0+1}\rangle) \right] && \why{inductive assm} \\
&= \E^{h, \pi, 1} \left[\rew +  \tilde{u}^{\pi}_t \right] && \why{\cref{thm:exp-horizon-cut}} \\
&= L_{\mathrm{h}}^{\pi} u_t^{\pi} && \why{\cref{def:DPhpi}} \\
&= u^{\pi}_t(h). && \why{\cref{def:u-dp-pi}}
\end{align*}
}
Also, we use $\tilde{u}_t^{\pi}$ to emphasize when we treat $u^{\pi}_t$ as a random variable. 
\end{proof}

The following theorem is akin to \citep[theorem~4.3.2]{Puterman2005}.
\begin{theorem}\label{thm:dph-opt-vf-opt}
For each $t\in \Nats$:
  \[
    u\opt_t(h)
    \ge 
    \hat{u}_t(h; \pi), \quad \forall h\in \mathcal{H}, \pi\in \PiHR.
  \]
  \lean{MDPs.dph_opt_vf_opt}
\end{theorem}
\begin{proof}
  By induction on $t$. The base case is immediate. The inductive case follows for $t+1$ as follows. For each $\pi\in \PiHR$:
  \begin{align*}
    u_{t+1}\opt(h)
    &= (L_{\mathrm{h}}\opt u_t\opt)(h) && \why{\cref{def:DPhopt}} \\
    &\ge (L_{\mathrm{h}}^{\pi} \hat{u}_t(\cdot; \pi))(h) && \why{ind asm, \cref{thm:dp-opt-ge-dp-pi}} \\
    &= (L_{\mathrm{h}}^{\pi} u_t^{\pi})(h) && \why{\cref{thm:dph-correct-vf}} \\
    &= u_t^{\pi}(h) && \why{\cref{def:u-pi}} \\
    &= \hat{u}_t(h; \pi). && \why{\cref{thm:dph-correct-vf}} \\
  \end{align*}
\end{proof}

\section{Expected Dynamic Program: Markov Policy}

\subsection{Optimality}
We discuss results needed to prove the optimality of Markov policies. 

\begin{definition} \label{def:Values}
  The set of \emph{independent value functions} is defined as $\mathcal{V} := \Real^{\mathcal{S}}$.
  \lean{MDPs.Values}\leanok
\end{definition}

\begin{definition}\label{def:DPMopt}
A \emph{Markov Bellman operator} $L\opt\colon \mathcal{V} \to \mathcal{V}$ is defined as
\[
(L\opt  v)(h)  :=
\max_{a\in \mathcal{A}} \E^{h, a, 1} \left[ \rew + v(\tilde{s}_{\mathrm{l}}) \right], \quad \forall \tilde{u} \in \mathcal{U},
\]
\lean{MDPs.DPMopt}\leanok
\end{definition}

\begin{definition} \label{def:v-dp-opt}
The \emph{optimal value function} $v\opt_t\in \mathcal{V}, t\in \Nats$ is defined as
\[
v\opt_t := 
\begin{cases}
    0 &\text{if } t = 0 \\
    (L\opt  v\opt _{t-1}) &\text{otherwise}.
\end{cases}
\]
\lean{MDPs.v_dp_opt}\leanok
\end{definition}

\begin{theorem}
Suppose that $t\in \Nats $. Then:
\[
v\opt_t(s_{\mathrm{l}}(h)) = u\opt_t(h), \quad \forall h\in \mathcal{H}.
\]
\lean{MDPs.v_dp_opt_eq_u_opt} 
\end{theorem}
\begin{proof}
By induction on $t$. The base case follows immediately from the definition. The inductive step for $t+1$ follows as:
\begin{align*}
  u\opt_{t+1}(h)
  &= \max_{a\in \mathcal{A}} \E^{h, a, 1} \left[ \rew_{|h|} + \tilde{u}\opt_t  \right]
  && \why{\cref{def:u-dp-opt}} \\
  &= \max_{a\in \mathcal{A}} \E^{h, a, 1} \left[ \rew_{|h|} + v\opt_t(\tilde{s}_l) \right] 
    && \why{inductive asm.}  \\
  &= \max_{a\in \mathcal{A}} \E^{s_0, a, 1} \left[ \rew_{|h|} + v\opt_t(\tilde{s}_l) \right]
    && \why{\cref{thm:exph-horizon-trim}} \\
  &= \max_{a\in \mathcal{A}} \E^{s_0, a, 1} \left[ \rew + v\opt_t(\tilde{s}_l) \right]
  && \why{\cref{thm:exph-congr}} \\
  &= v\opt_{t+1}(s_{\mathrm{l}}(h)) && \why{\cref{def:v-dp-opt}}.
\end{align*}
\end{proof}


\begin{definition} \label{def:pi-opt}
  The \emph{optimal finite-horizon policy} $\pi\opt_t, t\in \Nats$ is defined as
  \[
    \pi\opt_t(k,s) :=
    \begin{cases}
      \arg\max_{a\in \mathcal{A}}  \E^{s, a, 1} \left[ \rew + v_{t-k}\opt (\tilde{s}_{\mathrm{l}}) \right]
      &\text{if } k \le t, \\
      a_0 &\text{otherwise},
    \end{cases}
  \]
  where $a_0$ is an arbitrary action.
  \lean{MDPs.πopt}\leanok
\end{definition}

\begin{theorem}
Assume a horizon $T\in \Nats$. Then:
\[
  v\opt_{T-|h|}(s_{\mathrm{l}}(h)) = u^{\pi\opt_{T-h}}_{T-|h|}(h), \quad
  \forall h\in \left\{ h\in \mathcal{H} \mid |h| \le T \right\}.
\]
\lean{MDPs.v_dp_opt_eq_u_opt} 
\end{theorem}
\begin{proof}
Fix some $T \in \Nats$. By induction on $k$ from $k = T$ to $k=0$. The base case is immediate from the definition. We prove the inductive case for $k-1$ from $k$ as
\begin{align*}
u^{\pi\opt_T}_{T-k + 1}(h)
  &=  \E^{h, \pi\opt_T, 1} \left[ \rew_k + \tilde{u}^{\pi\opt_T}_{T-k} \right]
  && \why{\cref{def:DPhpi}} \\
  &=  \E^{h, a\opt, 1} \left[ \rew_k + \tilde{u}^{\pi\opt_T}_{T-k} \right]
  && \why{???} \\
  &=  \E^{h, a\opt, 1} \left[ \rew_{k} + v_{T-k}\opt(\tilde{s}_1) \right]
  && \why{ind asm} \\
  &=  \E^{s_0, a\opt, 1} \left[ \rew +  v_{T-k}\opt(\tilde{s}_1) \right]
  && \why{\cref{thm:exph-horizon-trim}} \\
  &=  \max_{a\in \mathcal{A}} \E^{s_0, a, 1} \left[ \rew +  v_{T-k}\opt(\tilde{s}_1) \right]
  && \why{???} \\
  &=  v_{T-k+1}\opt(s_0). && \why{\cref{def:v-dp-opt}}
\end{align*}
Here, $k := |h|$, $a\opt := \pi\opt_T(k, s_0)$, and $s_0 := s_{\mathrm{l}}(h)$
\end{proof}

\subsection{Evaluation}
We discuss results pertinent to the evaluation of Markov policies. 

Markov value functions depend on the length of the history.
\begin{definition} \label{def:ValuesM}
  The set of \emph{independent value functions} is defined as $\mathcal{V}_{\mathrm{M}} := \Real^{\Nats \times \mathcal{S}}$.
  \lean{MDPs.ValuesM}\leanok
\end{definition}


\begin{definition}\label{def:DPMopt}
A \emph{Markov policy Bellman operator} $L^{\pi}\colon \mathcal{V}_{\mathrm{M}} \to \mathcal{V}_{\mathrm{M}}$ for $\pi\in \Pi$ is defined as
\[
(L^{\pi} v)(k, s)  :=
\max_{a\in \mathcal{A}} \E^{s, a, 1} \left[ \rew + v(k + 1, \tilde{s}_{\mathrm{l}}) \right], \quad \forall v\in \mathcal{V}_{\mathrm{M}}, k\in \Nats, s\in \mathcal{S}.
\]
\lean{MDPs.DPMopt}\leanok
\end{definition}

\begin{definition} \label{def:v-dp-pi}
  The \emph{Markov policy value function} $v^{\pi}_t\in \mathcal{V}_{\mathrm{M}}, t \in \Nats$  for $\pi\in \PiMD$ is defined as
  \[
    v^{\pi}_t :=
    \begin{cases}
      0 &\text{if } t = 0, \\
      (L^{\pi}  v^{\pi}_{t-1}) &\text{otherwise}.
    \end{cases}
  \]
  \lean{MDPs.v_dp_π}\leanok
\end{definition}


\bibliography{refs}

% \section{Dynamic Program: Stationary}

% \subsection{Turnpikes}

%%% Local Variables:
%%% coding: utf-8
%%% mode: LaTeX
%%% TeX-master: "print"
%%% TeX-engine: xetex
%%% End:
\label{def:uopt}
