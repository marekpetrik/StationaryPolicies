% In this file you should put the actual content of the blueprint.
% It will be used both by the web and the print version.
% It should *not* include the \begin{document}
%
% If you want to split the blueprint content into several files then
% the current file can be a simple sequence of \input. Otherwise It
% can start with a \section or \chapter for instance.


\section{Probability Spaces and Expectation}

In this section, we define the basic probability concepts on finite sets. Eventually, this section should be replaced by the Mathlib results using probability theory and measure theory.

\begin{definition}[Probability Measure] \label{def:probability-measure}
  For a finite set $\Omega$, $\Delta(\Omega)$, a finite probability measure is  $p\colon \Omega \to \Real_+$ such that
  \[
   \sum_{\omega \in \Omega } p(\omega) = 1. 
 \]
 The $\sigma$-algebra defined for this set is the power set of $\Omega$.
  \lean{Findist} 
  \leanok
\end{definition}

\begin{definition}[Probability Space] \label{def:probability-space}
  A finite probability space comprises a finite set $\Omega$ along with a finite probability measure defined in \cref{def:probability-measure}.
  \lean{Fin}
  \leanok
\end{definition}

\begin{definition}[Expectation]
  The expectation of a random variable $\tilde{x} \colon \Omega \to \Real$ defined on a probability space $(\Omega, p)$ is defined as
  \[
    \E \left[ \tilde{x} \right] := \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega).
  \]
  \lean{Finprob.expect}
  \leanok
\end{definition}

\begin{definition}[Indicator]
  An indicator random variable $\mathbb{I}\colon  \Omega \to \left\{ 0, 1 \right\}$ is defined as
  \[
    \mathbb{I}(a) :=
    \begin{cases}
      1 &\text{if } a = \operatorname{true}, \\
      0 &\text{if } a = \operatorname{false}.
    \end{cases}
  \]
  \lean{Finprob.𝕀}
  \leanok
\end{definition}

\begin{definition}[Probability]
  The probability of $\tilde{b}\colon \Omega \to \mathcal{B}$ on a probability space $P = (\Omega, p)$ is defined as
  \[
    \P \left[ \tilde{b} \right] := \E\left[\mathbb{I}\left\{ \tilde{b} \right\}\right],
  \]
where $x / 0 = 0$ for each $x\in \Real$ (see \texttt{div\_zero}).
\end{definition}

\begin{definition}[Conditional Expectation]
  The conditional expectation of a random variable $\tilde{x} \colon \Omega \to \Real$ given an indicator $\tilde{c} \colon \Omega \to  \left\{ 0, 1  \right\}$ defined on a probability space $(\Omega, p)$ is defined as
  \[
    \E \left[ \tilde{x} \mid  \tilde{c} \right] := \frac{1}{\P[\tilde{c}]} \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega) \cdot \tilde{c}(\omega),
  \]
where $x / 0 = 0$ for each $x\in \Real$ (see \texttt{div\_zero}).
  \lean{Finprob.expect_cnd}
  \leanok
\end{definition}

\begin{definition}[Conditional Expectation RV]
  The conditional expectation of a random variable $\tilde{x} \colon \Omega \to \Real$ given a random variable $\tilde{y} \colon \Omega \to \mathcal{Y}$ for a finite $\mathcal{Y}$ is a random variable $\E \left[ \tilde{x} \mid  \tilde{y} \right]\colon \Omega \to \Real$  defined as
  \[
    \E \left[ \tilde{x} \mid  \tilde{y} \right](\omega)
    :=
    \frac{1}{\P[\tilde{y} = \tilde{y}(\omega)]} \sum_{\omega' \in \Omega } p(\omega) \cdot \tilde{x}(\omega') \cdot \mathbb{I}\left\{ \tilde{y}(\omega') = \tilde{y}(\omega) \right\}, \quad \forall \omega \in \Omega.
  \]
where $x / 0 = 0$ for each $x\in \Real$ (see \texttt{div\_zero}).
  \lean{Finprob.expect_cnd_rv}
  \leanok
\end{definition}





\begin{theorem}[Law of Total Expectation]
Any two random variables $\tilde{x} \colon \Omega \to \Real$ and $\tilde{y} \colon \Omega \to \mathcal{Y} $ for $\mathcal{Y}$ and probabiity space $(\Omega, p)$ satisfy for a finite set $\mathcal{Y}$ that
  \[
   \E\left[\E\left[\tilde{x} \mid  \tilde{y}\right]\right] = \E \left[ \tilde{x} \right] 
 \]
 \lean{Finprob.total_expectation}
\end{theorem}
\begin{proof}
  \begin{align*}
    \E\left[\E\left[\tilde{x} \mid  \tilde{y}\right]\right]
    &= \sum_{\omega \in \Omega } \E\left[\tilde{x} \mid  \tilde{y}\right](\omega) p(\omega)  \\
    &= \sum_{\omega \in \Omega } \sum_{\omega'\in \Omega} \tilde{x}(\omega) \mid  \tilde{y}(\omega) p(\omega)  \\
  \end{align*}
\end{proof}


\section{Markov Decision Process and Histories}

The basic probability space as defined as follows.
\begin{definition}[Markov Decision Process]
  A Markov decision process $M := (\mathcal{S}, \mathcal{A}, p, r)$ consists of a finite set of states $\mathcal{S}$, a finite set of actions $\mathcal{A}$, transition function $p\colon \mathcal{S} \times \mathcal{A} \times \Delta(\mathcal{S})$, and a reward function $r \colon \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \Real$.
  \lean{MDP}
  \leanok
\end{definition}

\begin{definition}[History]
  A history $h$ in $\mathcal{H}$ is a sequence of states and actions defined for an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$ and each horizon $T \in \Nats$:
  \[
    h := (s_0, a_0, s_1, a_1, \dots , s_T),
  \]
  where $s_k \in \mathcal{S}$ and $a_k\in \mathcal{A}$ for $k = 0, \dots , T-1$.
  \lean{Hist}
  \leanok
\end{definition}

\begin{definition}[Histories]
  The set of histories $\mathcal{H}_T \colon  \mathcal{H} \to 2^{\mathcal{H}}$ for an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$, following a history $\hat{h}$, is defined for each horizon $T \in \Nats$ as
  \[
    \mathcal{H}_T :=
    \begin{cases}
        \{ \hat{h} \} &\text{ if } T = 0 \\      
        \left\{ \langle h, a, s \rangle \mid h \in \mathcal{H}_{T-1}, a\in \mathcal{A}, s\in \mathcal{S} \right\} &\text{ if } T > 0.
    \end{cases}
  \]
  Here, $\langle \cdot , \cdot , \cdot  \rangle$ is an append operator to augments the history with the action and state.
  \lean{Histories}
  \leanok
\end{definition}

\begin{definition}[History-dependent randomized policies]
  For an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$, the set of history-dependent policies is $\Pi_{\mathrm{HR}} := \mathcal{S} \to \Delta(\mathcal{A})$.
  \lean{PolicyHR}
  \leanok
\end{definition}

\begin{definition}[History distribution]
  The probability distribution (\cref{def:probability-space}) $p^{\mathrm{h}}_T \in \Delta(\mathcal{H}_T(\hat{h}))$ over histories $\mathcal{H}_T(\hat{h})$ for an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$ and $\pi \in \Pi_{\mathrm{HR}}$ is defined for each $T \in \Nats$ and $h\in \mathcal{H}_T(\hat{h})$ as
  \[
    p^{\mathrm{h}}_T(h) :=
    \begin{cases}
      1 & \text{ if } T = 0 \wedge h = \hat{h}, \\
      0 & \text{ if } T = 0 \wedge h \neq  \hat{h}, \\
      p_{T-1}(h') \cdot \pi(h',a) \cdot  p(s, a , s') &\text{if } T > 1 \wedge h = \langle h', a, s' \rangle \wedge h' = \langle \cdot , \cdot , s \rangle.
    \end{cases}
  \]
  \lean{HistDist}
  \leanok
\end{definition}

\begin{definition}[History reward]
  The reward $r_{\mathrm{h}}\colon \mathcal{H} \to \Real$ for a history $h \in  \mathcal{H}$,
  \[
   h = (s_0, a_0, s_1, \dots , s_t), 
  \]
  is defined as
  \[
   r^{\mathrm{h}}(h) := \sum_{k=0}^{t-1} r(s_k, a_k, s_{k+1}).
 \]
 \lean{reward}
 \leanok
\end{definition}

\begin{definition}[History expectation]
  For each $T\in \Nats$, $\pi\in \Pi_{\mathrm{HR}}$, a history $\hat{h}\in \mathcal{H}$ and a random variable $\tilde{x}\colon \mathcal{H} \to \Real$ is defined as
  \[
   \E^{\pi, \hat{h}, T} [\tilde{x}]  := \sum_{h\in \mathcal{H}_T(\hat{h})} p^{\mathrm{h}}(h) \cdot x(h).
 \]
 \lean{}
\end{definition}

\section{History-Dependent DP}

Here, we derive dynamic programming equations for histories.

\section{Markov Value Functions}


\section{Markov Policies}

\section{Turnpikes}

%%% Local Variables:
%%% coding: utf-8
%%% mode: LaTeX
%%% TeX-master: "print"
%%% TeX-engine: xetex
%%% End:
