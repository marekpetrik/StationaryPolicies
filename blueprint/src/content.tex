% In this file you should put the actual content of the blueprint.
% It will be used both by the web and the print version.
% It should *not* include the \begin{document}
%
% If you want to split the blueprint content into several files then
% the current file can be a simple sequence of \input. Otherwise It
% can start with a \section or \chapter for instance.


\section{Probability Spaces and Expectation} \label{sec:prob-spac-expect}

In this section, we define the basic probability concepts on finite sets.
% Eventually, this section should be replaced by the Mathlib results using probability theory and measure theory.

\subsection{Definitions}

\begin{remark}
This document omits basic intuitive definitions, such as the comparison or arithmetic operations on random variables. Arithmetic operations on random variables are performed element-wise for each element of the sample set. Please see the Lean file for complete details.
\end{remark}

\begin{definition} \label{def:probability-measure}
A \emph{finite probability measure} $p\colon \Omega \to \Real_+$ on a finite set $\Omega$ is any function that satisfies
\[
\sum_{\omega \in \Omega } p(\omega) = 1. 
\]
\lean{Findist} \leanok
\end{definition}

\begin{definition}
The set of \emph{finite probability measures} $\Delta(\Omega)$ for a finite $\Omega$ is defined as
\[
\Delta(\Omega) := \left\{ p\colon \Omega \to \Real_+ \mid  \sum_{\omega \in \Omega } p(\omega) = 1 \right\}.
\]
\lean{Delta} \leanok
\end{definition}

\begin{definition} \label{def:probability-space}
A \emph{finite probability space} is $P = (\Omega, p)$, where $\Omega$ is a finite set referred to as the \emph{sample set}, $p\in \Delta(\Omega)$, and the $\sigma$-algebra is $2^{\Omega}$.
\lean{Finprob} \leanok
\end{definition}

\begin{definition}
A \emph{random variable} defined on a finite probability space $P$ is a mapping $\tilde{x}\colon \Omega \to \Real$.
\lean{Finrv} \leanok
\end{definition}

For the remainder of \cref{sec:prob-spac-expect}, we assume that $P = (\Omega, p)$ is a \emph{finite probability space}. All random variables are defined on the space $P$ unless specified otherwise.

\begin{definition}
A \emph{boolean} set is $\mathcal{B} = \left\{ \false, \true \right\}$.
\lean{Bool} \leanok
\end{definition}

\begin{definition} \label{def:expect}
The \emph{expectation} of a random variable $\tilde{x} \colon \Omega \to \Real$ is 
\[
\E \left[ \tilde{x} \right] := \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega).
\]
\lean{Finprob.expect} \leanok
\end{definition}

\begin{definition} \label{def:indicator}
An \emph{indicator} function $\I \colon \mathcal{B} \to \left\{ 0, 1 \right\}$ is defined for $b\in \mathcal{B}$ as
\[
\I(b) :=
\begin{cases}
1 &\text{if } b = \operatorname{true}, \\
0 &\text{if } b = \operatorname{false}.
\end{cases}
\]
\lean{Finprob.indicator} \leanok
\end{definition}

\begin{definition} \label{def:probability}
The \emph{probability} of $\tilde{b}\colon \Omega \to \mathcal{B}$ is defined as
\[
\PP\left[ \tilde{b} \right] := \E\left[\I(\tilde{b})\right].
\]
\lean{Finprob.probability}
\end{definition}

\begin{definition} \label{def:expect-cnd}
The \emph{conditional expectation} of $\tilde{x} \colon \Omega \to \Real$ conditioned on $\tilde{c} \colon \Omega \to \mathcal{B}$ is defined as
\[
\E \left[ \tilde{x} \mid  \tilde{b} \right] :=
\frac{1}{\PP[\tilde{c}]} \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega) \cdot \I(\tilde{c}(\omega)),
\]
where we define that $x / 0 = 0$ for each $x\in \Real$.
\lean{Finprob.expect_cnd} \leanok
\end{definition}

\begin{definition} \label{def:probability_cnd}
The \emph{conditional probability} of $\tilde{b}\colon \Omega \to \mathcal{B}$ on $\tilde{c}\colon \Omega \to \mathcal{B}$ is defined as
\[
\PP\left[ \tilde{b} \mid  \tilde{c} \right]  :=
\E \left[ \I(\tilde{b}) \mid  \tilde{c} \right].
\]
\end{definition}

\begin{remark}
It is common to prohibit conditioning on a zero probability event both for expectation and probabilities. In this document, we follow the Lean convention, where the division by $0$ is $0$; see \texttt{div\_zero}. However, even some basic probability and expectation results may require that we assume that the conditioned event does not have probability zero for it to hold. 
\end{remark}

\begin{definition} \label{def:expect-cnd-rv}
The \emph{random conditional expectation} of a random variable $\tilde{x} \colon \Omega \to \Real$ conditioned on $\tilde{y} \colon \Omega \to \mathcal{Y}$ for a finite set $\mathcal{Y}$ is the random variable $\E \left[ \tilde{x} \mid  \tilde{y} \right]\colon \Omega \to \Real$ is defined as
\[
\E \left[ \tilde{x} \mid  \tilde{y} \right](\omega)
:=
\E \left[ \tilde{x} \mid  \tilde{y} = \tilde{y}(\omega) \right], \quad \forall \omega \in \Omega.
\]
\lean{Finprob.expect_cnd_rv} \leanok
\end{definition}

\begin{remark}
The Lean file defines expectations more broadly for a data type $\rho$ which is more general than just $\Real$. The main reason to generalize to both $\Real$ and $\Real_+$. However, in principle, the definitions could be used to reason with expectations that go beyond real numbers and may include other algebras, such as vectors or matrices.
\end{remark}

\subsection{Basic Properties}

\begin{lemma} \label{lem:ind-and-eq-prod-ind}
Suppose that $\tilde{b}, \tilde{c} \colon \Omega \to \mathcal{B}$. Then:
\[
\I \left( \tilde{b} \wedge \tilde{c} \right) = \I ( \tilde{b} ) \cdot  \I ( \tilde{c} ),  
\]
where the equality applies for all $\omega \in \Omega$.
\end{lemma}

\begin{theorem} \label{thm:exp-zero-cond}
Suppose that $\tilde{c} \colon \Omega \to \mathcal{B}$ such that $\PP\left[ \tilde{c} \right] = 0$. Then for any $\tilde{x} \colon \Omega \to \Real$:
\[
\E \left[ \tilde{x} \mid \tilde{c} \right] = 0.
\]
\end{theorem}
\begin{proof}
Immediate from the definition and the fact that $0 \cdot  x = 0$ for $x\in \Real$.
\end{proof}

\begin{theorem} \label{thm:prob-zero-cond}
Suppose that $\tilde{c} \colon \Omega \to \mathcal{B}$ such that $\PP\left[ \tilde{c} \right] = 0$. Then for any $\tilde{b} \colon \Omega \to \Real$:
\[
\PP\left[ \tilde{b} \mid \tilde{c} \right] = 0.
\]
\end{theorem}
\begin{proof}
Immediate from \cref{thm:exp-zero-cond}.
\end{proof}

\begin{theorem} \label{thm:prob-eq-prob-cond-prob}
Suppose that $\tilde{b}, \tilde{c} \colon \Omega \to \mathcal{B}$, then
\[
\PP \left[ \tilde{b} \wedge \tilde{c} \right] 
=
\PP \left[ \tilde{b} \mid \tilde{c} \right] \cdot \PP \left[ \tilde{c} \right].
\]
\end{theorem}
\begin{proof} The property holds immediately when $\PP \left[ \tilde{c} \right] = 0$. Assume that $\PP \left[ \tilde{c} \right] > 0$. Then:
  \begin{align*}
    \PP \left[ \tilde{b} \wedge \tilde{c} \right]
    &= \E \left[ \I(\tilde{b} \wedge \tilde{c}) \right] && \text{\cref{def:probability}} \\
    &= \E \left[ \I(\tilde{b}) \cdot \I(\tilde{c}) \right] && \text{\cref{lem:ind-and-eq-prod-ind}}\\
    &= \frac{1}{\PP \left[ \tilde{c} \right]}\E \left[ \I(\tilde{b}) \cdot \I(\tilde{c}) \right] \cdot \PP \left[ \tilde{c} \right] && \cdot 1 \\
    &= \E \left[ \I(\tilde{b}) \mid \tilde{c} \right] \cdot \PP \left[ \tilde{c} \right]
    && \text{\cref{def:expect-cnd}} \\
    &= \PP \left[ \tilde{b} \mid \tilde{c} \right] \cdot \PP \left[ \tilde{c} \right] && \text{\cref{def:probability_cnd}}.
  \end{align*}
\end{proof}

\subsection{The Laws of The Unconscious Statisticians}

\begin{theorem} \label{thm:exp-sum-val}
Let $\tilde{x} \colon \Omega \to \Real $ be a random variable. Then:
\[
\E \left[ \tilde{x} \right]
=
\sum_{x\in \tilde{x}(\Omega)} \PP \left[ \tilde{x} = x \right] \cdot  x. 
\]
\lean{Finprob.exp_sum_val}
\end{theorem}
\begin{proof}
Let $\mathcal{X} := \tilde{x}(\Omega)$, which is a finite set. Then:
\begin{align*}
\E \left[ \tilde{x} \right]
&= \sum_{\omega \in \Omega } p(\omega ) \cdot \tilde{x}(\omega) && \text{\cref{def:expect}} \\
&= \sum_{\omega \in \Omega } \sum_{x\in \mathcal{X}} p(\omega ) \cdot \tilde{x}(\omega) \cdot \I(x =  \tilde{x}(\omega)) && \text{??} \\
&= \sum_{\omega \in \Omega } \sum_{x\in \mathcal{X}} p(\omega ) \cdot x \cdot \I(x =  \tilde{x}(\omega)) && \text{??} \\
&= \sum_{x\in \mathcal{X}} x \cdot \sum_{\omega \in \Omega }  p(\omega ) \cdot  \I(x =  \tilde{x}(\omega))  && \text{??}\\
&= \sum_{x\in \mathcal{X}} x \cdot \E \left[  \I(x =  \tilde{x}(\omega)) \right] && \text{\cref{def:expect}} \\
&= \sum_{x\in \mathcal{X}} x \cdot \PP \left[x =  \tilde{x}(\omega) \right]. && \text{\cref{def:probability}} 
\end{align*}
\end{proof}

The following theorem generalizes the theorem above. 
\begin{theorem}\label{thm:exp-sum-val-cnd}
Let $\tilde{x} \colon \Omega \to \Real$ and $\tilde{b} \colon \Omega \to \mathcal{Y}$ be random variables. Then:
\[
  \E \left[ \tilde{x} \mid \tilde{b} \right] 
  =
  \sum_{x\in \tilde{x}(\Omega)} \PP \left[ \tilde{x} = x \mid \tilde{b}  \right] \cdot x.
\]
\lean{Finprob.exp_sum_val_cnd}
\end{theorem}

\begin{theorem} \label{thm:exp-sum-val-cnd-rv}
Let $\tilde{x} \colon \Omega \to \Real$ and $\tilde{y} \colon \Omega \to \mathcal{Y}$ be random variables with $\mathcal{Y}$ finite. Then:
\[
  \E \left[ \E \left[ \tilde{x} \mid  \tilde{y} \right] \right]
  =
  \sum_{y\in \mathcal{Y}} \E \left[ \tilde{x} \mid  \tilde{y} = y \right] \cdot
                       \PP\left[ \tilde{y} = y \right].
\]
\lean{Finprob.exp_sum_val_cnd_rv}
\end{theorem}


\subsection{Total Expectation and Probability}


\begin{theorem}[Law of Total Probability] \label{thm:total-probability}
Let $\tilde{b} \colon \Omega \to \mathcal{B}$ and $\tilde{y} \colon \Omega \to \mathcal{Y}$ be random variables with a finite set $\mathcal{Y}$. Then:
\[
\sum_{y\in \mathcal{Y}} \PP\left[\tilde{b} \wedge \tilde{y} = y \right] = \PP \left[ \tilde{b} \right].
\]
\lean{Finprob.total_expectation}
\end{theorem}

\begin{theorem}[Law of Total Expectation] \label{thm:total_expectation}
Let $\tilde{x} \colon \Omega \to \mathcal{X}$ and $\tilde{y} \colon \Omega \to \mathcal{Y}$ be random variables with a finite set $\mathcal{Y}$. Then:
\[
\E\left[\E\left[\tilde{x} \mid  \tilde{y}\right]\right] = \E \left[ \tilde{x} \right].
\]
\lean{Finprob.total_expectation}
\end{theorem}
\begin{proof}
Recall that we are allowing the division by 0 and assume that $x / 0 = 0$.
\begin{align*} 
  \E\left[\E\left[\tilde{x} \mid  \tilde{y}\right]\right]
  &= \sum_{\omega \in \Omega }  p (\omega) \cdot  \E \left[ \tilde{x} \mid  \tilde{y} \right](\omega) && \text{\cref{def:expect}}\\
  &= \sum_{\omega \in \Omega }  p (\omega) \cdot  \E \left[ \tilde{x} \mid  \tilde{y}  = \tilde{y}(\omega) \right] && \text{~\cref{def:expect-cnd-rv}}\\
  &= \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y}= \tilde{y}(\omega)\right]} \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega') \cdot  \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right)   && \text{\cref{def:expect-cnd}} \\
  &=  \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega') \cdot \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y}= \tilde{y}(\omega)\right]} \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right)  && \text{???, rearrange} \\
  &=  \sum_{\omega'\in \Omega } p(\omega') \cdot \tilde{x}(\omega') \cdot \sum_{\omega \in \Omega }  \frac{p(\omega)}{\PP \left[ \tilde{y} = \tilde{y}(\omega')\right]} \I\left( \tilde{y}(\omega') = \tilde{y}(\omega) \right)  && \text{equals when non-zero} \\
  &= ????????? \\
  &= \E \left[ \tilde{x} \right].
\end{align*}
\end{proof}
The following proof is simpler but may require some more advanced properties.
\begin{proof}[Alternate proof]
\begin{align*}
\E\left[\E\left[\tilde{x} \mid  \tilde{y}\right] \right]
&= \sum_{y\in \mathcal{Y}} \E\left[\tilde{x} \mid \tilde{y} = y \right] \cdot \PP\left[ \tilde{y} = y \right]  \\
&= \sum_{y\in \mathcal{Y}} \sum_{x\in \mathcal{X}} x \cdot \PP\left[\tilde{x} = x \mid \tilde{y} = y \right] \cdot \PP\left[ \tilde{y} = y \right]  \\
&= \sum_{y\in \mathcal{Y}} \sum_{x\in \mathcal{X}} x \cdot  \PP\left[\tilde{x} = x, \tilde{y} = y \right]   \\
&=  \sum_{x\in \mathcal{X}} x \cdot \sum_{y\in \mathcal{Y}} \PP\left[\tilde{x} = x, \tilde{y} = y \right]   \\
&=  \sum_{x\in \mathcal{X}} x \cdot  \PP\left[\tilde{x} = x \right]  = \E \left[ \tilde{x} \right].
\end{align*}
\end{proof}

\section{Markov Decision Process and Histories}

\begin{definition}[Markov Decision Process]
  A Markov decision process $M := (\mathcal{S}, \mathcal{A}, p, r)$ consists of a finite set of states $\mathcal{S}$, a finite set of actions $\mathcal{A}$, transition function $p\colon \mathcal{S} \times \mathcal{A} \times \Delta(\mathcal{S})$, and a reward function $r \colon \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \Real$.
\lean{MDP} \leanok
\end{definition}

\begin{definition}[History]
A history $h$ in $\mathcal{H}$ is a sequence of states and actions defined for an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$ and each horizon $T \in \Nats$:
\[
h := (s_0, a_0, s_1, a_1, \dots , s_T),
\]
where $s_k \in \mathcal{S}$ and $a_k\in \mathcal{A}$ for $k = 0, \dots , T-1$.
\lean{Hist} \leanok
\end{definition}

\begin{definition}[Histories]
  The set of histories $\mathcal{H}_T \colon  \mathcal{H} \to 2^{\mathcal{H}}$ for an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$, following a history $\hat{h}$, is defined for each horizon $T \in \Nats$ as
  \[
    \mathcal{H}_T :=
    \begin{cases}
        \{ \hat{h} \} &\text{ if } T = 0 \\      
        \left\{ \langle h, a, s \rangle \mid h \in \mathcal{H}_{T-1}, a\in \mathcal{A}, s\in \mathcal{S} \right\} &\text{ if } T > 0.
    \end{cases}
  \]
  Here, $\langle \cdot , \cdot , \cdot  \rangle$ is an append operator to augments the history with the action and state.
  \lean{Histories}
  \leanok
\end{definition}

\begin{definition}[History-dependent randomized policies]
  For an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$, the set of history-dependent policies is $\Pi_{\mathrm{HR}} := \mathcal{S} \to \Delta(\mathcal{A})$.
  \lean{PolicyHR}
  \leanok
\end{definition}

\begin{definition}[History distribution]
  The probability distribution (\cref{def:probability-space}) $p^{\mathrm{h}}_T \in \Delta(\mathcal{H}_T(\hat{h}))$ over histories $\mathcal{H}_T(\hat{h})$ for an MDP $M = (\mathcal{S}, \mathcal{A}, p, r)$ and $\pi \in \Pi_{\mathrm{HR}}$ is defined for each $T \in \Nats$ and $h\in \mathcal{H}_T(\hat{h})$ as
  \[
    p^{\mathrm{h}}_T(h) :=
    \begin{cases}
      1 & \text{ if } T = 0 \wedge h = \hat{h}, \\
      0 & \text{ if } T = 0 \wedge h \neq  \hat{h}, \\
      p_{T-1}(h') \cdot \pi(h',a) \cdot  p(s, a , s') &\text{if } T > 1 \wedge h = \langle h', a, s' \rangle \wedge h' = \langle \cdot , \cdot , s \rangle.
    \end{cases}
  \]
  \lean{HistDist}
  \leanok
\end{definition}

\begin{definition}[History reward]
  The reward $r_{\mathrm{h}}\colon \mathcal{H} \to \Real$ for a history $h \in  \mathcal{H}$,
  \[
   h = (s_0, a_0, s_1, \dots , s_t), 
  \]
  is defined as
  \[
   r^{\mathrm{h}}(h) := \sum_{k=0}^{t-1} r(s_k, a_k, s_{k+1}).
 \]
 \lean{reward}
 \leanok
\end{definition}

\begin{definition}[History expectation]
  For each $T\in \Nats$, $\pi\in \Pi_{\mathrm{HR}}$, a history $\hat{h}\in \mathcal{H}$ and a random variable $\tilde{x}\colon \mathcal{H} \to \Real$ is defined as
  \[
   \E^{\pi, \hat{h}, T} [\tilde{x}]  := \sum_{h\in \mathcal{H}_T(\hat{h})} p^{\mathrm{h}}(h) \cdot x(h).
 \]
 \lean{}
\end{definition}

\section{History-Dependent DP}

Here, we derive dynamic programming equations for histories.

\section{Markov Value Functions}


\section{Markov Policies}

\section{Turnpikes}

%%% Local Variables:
%%% coding: utf-8
%%% mode: LaTeX
%%% TeX-master: "print"
%%% TeX-engine: xetex
%%% End:
