import LeanMDPs.Histories

--set_option diagnostics true
--set_option diagnostics.threshold 3

open NNReal
open Finprob
--open MDPs


section ArgMax

variable {Ï„ : Type*} {a m : Ï„} {S : Finset Ï„} {f : S â†’ â„}

noncomputable 
def Finset.argmax' (I: Finset Ï„) (H : I.Nonempty) (f : I â†’ â„)  : I :=
  -- TODO: make it implemtable by directly applying fold as in Finset.sum
  let M := List.argmax f I.attach.toList
  let H1 : I.attach.toList â‰  [] := Finset.Nonempty.toList_ne_nil (Finset.attach_nonempty_iff.mpr H)
  let H2 : M â‰  none := fun eq â†¦ H1 (List.argmax_eq_none.mp eq)
  match M, H2 with 
  | (x:I),_ => x

#check List.argmax

theorem le_of_mem_argmax'  
  (h : a âˆˆ S) (NE : S.Nonempty) : f âŸ¨a, hâŸ© â‰¤ f (S.argmax' NE f)  := 
     let m := S.argmax' NE f
     sorry

theorem argmax_eq_sup (NE : S.Nonempty) : 
    S.attach.sup' (Finset.attach_nonempty_iff.mpr NE) f = f (S.argmax' NE f) := sorry

example : a âˆˆ S.toList â†” a âˆˆ S := Finset.mem_toList
example (h : a âˆˆ S) : âŸ¨a,hâŸ© âˆˆ S.attach  := Finset.mem_attach S âŸ¨a, hâŸ©

end ArgMax

namespace MDPs

/- state -/
variable {Ïƒ : Type} [Inhabited Ïƒ] [DecidableEq Ïƒ] 
/- action -/
variable {Î± : Type} [Inhabited Î±] [DecidableEq Î±]
variable {M : MDP Ïƒ Î±}

section Objectives

/-- Finite horizon objective parameters -/
structure ObjectiveFH (M : MDP Ïƒ Î±) : Type where
  sâ‚€ : M.S -- initial state
  T : â„• -- horizon

/-- Objective function -/
def objective_fh (O : ObjectiveFH M) (Ï€ : PolicyHR M) := ð”¼â‚•[ reward // O.sâ‚€, Ï€, O.T]

/-- An optimal policy Ï€opt -/
def Optimal_fh (O : ObjectiveFH M) (Ï€opt : PolicyHR M) := âˆ€ Ï€ : PolicyHR M, objective_fh O Ï€opt â‰¥ objective_fh O Ï€

/-- Value function type for history-dependent value functions -/
def ValuesH (M : MDP Ïƒ Î±) : Type := Hist M â†’ â„

/-- History-dependent value function -/
def hvalue_Ï€ (Ï€ : PolicyHR M) : â„• â†’ ValuesH M
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => fun h â†¦ ð”¼â‚•[ reward_from h.length // h,Ï€,t.succ ] 
  
-- TODO: This needs some thought to be defined properly
--def hvalue_opt : â„• â†’ ValuesH M
--    | Nat.zero => fun _ â†¦ 0
--    | Nat.succ t => fun h => fun Ï€ â†¦ hvalue_Ï€ Ï€  

/-- A value-optimal policy Ï€opt -/
def OptimalVF_fh (t : â„•) (Ï€opt : PolicyHR M) := âˆ€ Ï€ : PolicyHR M, âˆ€ h : Hist M, hvalue_Ï€ Ï€opt t h â‰¥ hvalue_Ï€ Ï€ t h

omit [Inhabited Ïƒ] [DecidableEq Ïƒ] [Inhabited Î±] [DecidableEq Î±] 
lemma reward_eq_reward_from_0 : âˆ€h : Hist M, reward h = reward_from 0 h :=      
           fun h => by induction h; simp! only []; simp_all!

theorem optimalvf_imp_optimal {O : ObjectiveFH M} 
  (Ï€opt : PolicyHR M) (opt : OptimalVF_fh O.T Ï€opt) : (Optimal_fh O Ï€opt) :=  
        fun Ï€ => calc
            objective_fh O Ï€opt = ð”¼â‚•[ reward // O.sâ‚€, Ï€opt, O.T] := rfl
            _ = ð”¼â‚•[ reward_from 0 // O.sâ‚€, Ï€opt, O.T] := 
                    exph_congr reward (reward_from 0) (fun h' a â†¦ reward_eq_reward_from_0 h')
            _ = hvalue_Ï€ Ï€opt O.T O.sâ‚€ := 
                sorry -- by cases O.T; simp![exph_zero_horizon_eq_zero_f]; simp_all!
            _ â‰¥ hvalue_Ï€ Ï€ O.T O.sâ‚€ := opt Ï€ (Hist.init â†‘O.sâ‚€)
            _ =  ð”¼â‚•[ reward_from 0 // O.sâ‚€, Ï€, O.T] := 
                    sorry --by cases O.T; simp![exph_zero_horizon_eq_zero_f]; dsimp!
            _ = ð”¼â‚•[ reward // O.sâ‚€, Ï€, O.T] := 
                exph_congr (reward_from 0) reward (fun h' a â†¦ (reward_eq_reward_from_0 h').symm)
            _ = objective_fh O Ï€ := rfl
                
end Objectives

section HistoryDP

/-- Bellman operator on history-dependent value functions -/
def DPhÏ€ (Ï€ : PolicyHR M) (v : ValuesH M) : ValuesH M 
  | h => ð”¼â‚•[ fun h' â†¦ reward h' + v h' // h, Ï€, 1 ]
  
/-- Bellman operator on history-dependent value functions -/
def DPhopt (u : ValuesH M) : ValuesH M 
  | h => let q (a : M.A) :=  ð”¼â‚•[fun h' â†¦ reward_at h.length h' + u h' // h, a, 1]
         M.A.attach.sup' (Finset.attach_nonempty_iff.mpr M.A_ne) q

--let q a :=  âˆ‘ s' âˆˆ M.S, (M.P h.last a).p s' * (M.r h.last a s' + vâ‚œ (h.foll a s'))

/-- Dynamic program value function, finite-horizon history dependent -/
def u_dp_Ï€ (Ï€ : PolicyHR M) : â„• â†’ ValuesH M 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPhÏ€ Ï€ (u_dp_Ï€ Ï€ t)


/-- Dynamic program value function, finite-horizon history dependent -/
def u_dp_opt  : â„• â†’ ValuesH M 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPhopt (u_dp_opt t)
  
theorem dp_opt_ge_dp_pi (h : Hist M) (uâ‚ uâ‚‚ : ValuesH M) (uge : âˆ€h : Hist M, uâ‚ h â‰¥ uâ‚‚ h) :
        âˆ€h : Hist M, âˆ€Ï€ : PolicyHR M, DPhopt uâ‚ h â‰¥ DPhÏ€ Ï€ uâ‚‚ h := sorry


theorem dph_correct_vf (Ï€ : PolicyHR M) (t : â„•) (h : Hist M) : hvalue_Ï€ Ï€ t h = u_dp_Ï€ Ï€ t h := 
   match t with
     | Nat.zero => rfl
     | Nat.succ t => 
       let hp h' := dph_correct_vf Ï€ t h'
       sorry

theorem dph_opt_vf_opt (t : â„•) : 
        âˆ€Ï€ : PolicyHR M, âˆ€ h : Hist M, u_dp_opt t h â‰¥ u_dp_Ï€ Ï€ t h := sorry 

end HistoryDP


section MarkovOptimality -- Markov policies and value functions as a dynamic program

/-- A deterministic Markov policy. Depends on the time step, 
and does not depend on the horizon. -/
def PolicyMD (M : MDP Ïƒ Î±) : Type := â„• â†’ DecisionRule M

instance [DecidableEq Î±] : Coe (PolicyMD M) (PolicyHR M) where
  coe d := fun h â†¦ dirac_dist M.A (d h.length h.last)

/-- History-independent value function. Note that the optimal
value function is history-independent, while the 
value function of a Markov policy depends on the time step. -/
def Values (_ : MDP Ïƒ Î±) := Ïƒ â†’ â„

/-- Markov q function -/
def q_of_v (s : Ïƒ) (a : M.A) (v : Values M) : â„ :=
 ð”¼â‚•[ fun h â†¦ reward h + v h.last // (s:Hist M), (a:PolicyHR M), 1 ]

/-- Bellman operator on history-dependent value functions -/
def DPMopt (v : Values M) : Values M
  | s => M.A.attach.sup' (Finset.attach_nonempty_iff.mpr M.A_ne) (fun a â†¦ q_of_v s a v)


/-- Optimal value function -/
def v_dp_opt : â„• â†’ Values M
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPMopt (v_dp_opt t)


variable {t : â„•}

/-- The Markov DP is optimal -/
theorem v_dp_opt_eq_u_opt : âˆ€h : Hist M, v_dp_opt (M:=M) t h.last = u_dp_opt t h := 
  sorry


/-- Optimal policy for horizon t -/
noncomputable
def Ï€opt (t : â„•) : PolicyMD M 
  | k, s => 
    if t â‰¥ k then 
       M.A.argmax' M.A_ne (fun a â†¦ q_of_v s a (v_dp_opt (t-k)) )
    else
      -- just return some action
      Classical.indefiniteDescription (fun a â†¦ a âˆˆ M.A) M.A_ne

/-- Greedy to v_opt is optimal policy -/
theorem v_dp_opt_eq_v_dp_Ï€ {T : â„•} : âˆ€h : Hist M, h.length â‰¤ T â†’ 
           v_dp_opt (M:=M) (T - h.length) h.last = u_dp_Ï€ (Ï€opt (M:=M) T) (T - h.length) h := 
        sorry

end MarkovOptimality

section MarkovEvaluation


def ValuesM (_ : MDP Ïƒ Î±) := â„• â†’ Ïƒ â†’ â„

/-- Optimal Bellman operator on state-dependent value functions. Also includes
the prior history's reward. -/
def DPMÏ€ (Ï€ : PolicyMD M) (v : ValuesM M) : ValuesM M 
  | k,s => ð”¼â‚•[ fun h â†¦ reward h + v (k+1) h.last // s, (Ï€ : PolicyHR M), 1 ]

/-- Value function of a Markov policy. Horizon to value function. -/
def v_dp_Ï€ (Ï€:PolicyMD M) : â„• â†’ ValuesM M
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPMÏ€ Ï€ (v_dp_Ï€ Ï€ t)

variable {t : â„•} {Ï€ : PolicyMD M}

theorem v_eq_u_Ï€ : âˆ€ h : Hist M, u_dp_Ï€ Ï€ t h = v_dp_Ï€ Ï€ t h.length h.last := sorry

theorem markov_u_quot : 
  âˆ€ hâ‚ hâ‚‚ : Hist M, hâ‚.length = hâ‚‚.length âˆ§ hâ‚.last = hâ‚‚.last â†’ u_dp_Ï€ Ï€ t hâ‚ = u_dp_Ï€ Ï€ t hâ‚‚ := 
        fun hâ‚ hâ‚‚ => fun a => by simp_all [v_eq_u_Ï€]

end MarkovEvaluation


end MDPs

