import LeanMDPs.Histories

variable {Ïƒ Î± : Type}
variable [Inhabited Ïƒ] [Inhabited Î±]
variable [DecidableEq Ïƒ] [DecidableEq Î±]

variable {m : MDP Ïƒ Î±}

/-- 
Value function type for value functions that are
- history-dependent
- for a specific policy
- and a horizon T
-/
def ValueH (m : MDP Ïƒ Î±) : Type := Hist m â†’ â„


/--
Defines an MDP Bellman operator on history-dependent value functions
-/
def DPÏ€ (Ï€ : PolicyHR m) (vâ‚œ : ValueH m) : ValueH m 
  | h => âˆ‘ a âˆˆ m.A, âˆ‘ s' âˆˆ m.S,  
           ((Ï€ h).p a * (m.P h.last a).p s') * (m.r h.last a s' + vâ‚œ h)

/--
Defines the value function for a fixed history-dependent policy and a
given horizon
-/
def value_Ï€ (Ï€ : PolicyHR m) : â„• â†’ ValueH m
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => fun hâ‚– â†¦ ð”¼â‚• hâ‚– Ï€ t.succ reward


def value_dp_Ï€ (Ï€ : PolicyHR m) : â„• â†’ ValueH m 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPÏ€ Ï€ (value_dp_Ï€ Ï€ t)

theorem dp_correct_vf  (Ï€ : PolicyHR m) (t : â„•) (h : Hist m) : value_Ï€ Ï€ t h = value_dp_Ï€ Ï€ t h := 
   match t with
     | Nat.zero => rfl
     | Nat.succ t' => sorry
              
