import LeanMDPs.Histories

--set_option diagnostics true
--set_option diagnostics.threshold 3

open NNReal
open Finprob
open MDPs

/- state -/
variable {Ïƒ : Type} [Inhabited Ïƒ] [DecidableEq Ïƒ] 
/- action -/
variable {Î± : Type} [Inhabited Î±] [DecidableEq Î±]
variable {M : MDP Ïƒ Î±}

section Objectives

-- Future generalization??
/- Generic objective definition -/
--def Objective (Ïƒ : Type) (Î± : Type) := MDP Ïƒ Î± â†’ Type
/- General definition of an objective function -/
--class ObjectiveFun (o : Objective Ïƒ Î±) where
--  obj : Phr M â†’ â„

/-- Finite horizon objective parameters -/
structure ObjectiveFH (M : MDP Ïƒ Î±) : Type where
  sâ‚€ : M.S -- initial state
  T : â„• -- horizon


/-- Objective function -/
def objective_fh (O : ObjectiveFH M) (Ï€ : Phr M) := ð”¼â‚•[ reward // O.sâ‚€, Ï€, O.T]

/-- An optimal policy Ï€opt -/
def Optimal_fh (O : ObjectiveFH M) (Ï€opt : Phr M) := âˆ€ Ï€ : Phr M, objective_fh O Ï€opt â‰¥ objective_fh O Ï€

/-- Value function type for value functions that are -/
def ValuesH (M : MDP Ïƒ Î±) : Type := Hist M â†’ â„

/-- History-dependent value function -/
def hvalue_Ï€ (Ï€ : PolicyHR M) : â„• â†’ ValuesH M
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => fun h â†¦ ð”¼â‚•[ reward // h, Ï€, t.succ ] - reward h
  

-- TODO: This needs some thought to be defined properly
--def hvalue_opt : â„• â†’ ValuesH M
--    | Nat.zero => fun _ â†¦ 0
--    | Nat.succ t => fun h => fun Ï€ â†¦ hvalue_Ï€ Ï€  


/-- A value-optimal policy Ï€opt -/
def OptimalVF_fh (t : â„•) (Ï€opt : Phr M) := âˆ€ Ï€ : Phr M, âˆ€ h : Hist M, hvalue_Ï€ Ï€opt t h â‰¥ hvalue_Ï€ Ï€ t h

theorem optimalvf_imp_optimal {O : ObjectiveFH M} (Ï€opt : PolicyHR M) (opt : OptimalVF_fh O.T Ï€opt) : 
                              (Optimal_fh O Ï€opt) := 
        fun Ï€ => 
            calc
                objective_fh O Ï€opt = ð”¼â‚•[ reward // O.sâ‚€, Ï€opt, O.T] := rfl
                _ = ð”¼â‚•[ reward // O.sâ‚€, Ï€opt, O.T ] - reward (O.sâ‚€ : Hist M) + reward (O.sâ‚€ : Hist M) := by ring
                _ = hvalue_Ï€ Ï€opt O.T O.sâ‚€ + reward (O.sâ‚€ : Hist M) := 
                             by cases O.T; simp_all! [exph_zero_horizon_eq_zero]; simp_all! only [sub_add_cancel]
                _ â‰¥ objective_fh O Ï€ := sorry
          

end Objectives

section DPValueH

/-- Bellman operator on history-dependent value functions -/
def DPhÏ€ (Ï€ : PolicyHR M) (vâ‚œ : ValuesH M) : ValuesH M 
  | h => âˆ‘ a âˆˆ M.A, âˆ‘ s' âˆˆ M.S, ((Ï€ h).p a * (M.P h.last a).p s') * (M.r h.last a s' + vâ‚œ h)

/-- Bellman operator on history-dependent value functions -/
def DPhopt (vâ‚œ : ValuesH M) : ValuesH M 
  | h => let q a :=  âˆ‘ s' âˆˆ M.S, (M.P h.last a).p s' * (M.r h.last a s' + vâ‚œ h)
         M.A.sup' M.A_ne q

/-- Dynamic program value function, finite-horizon history dependent -/
def u_dp_Ï€ (Ï€ : PolicyHR M) : â„• â†’ ValuesH M 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPhÏ€ Ï€ (u_dp_Ï€ Ï€ t)


/-- Dynamic program value function, finite-horizon history dependent -/
def u_dp_opt  : â„• â†’ ValuesH M 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPhopt (u_dp_opt t)
  
theorem dp_opt_ge_dp_pi (h : Hist M) (uâ‚ uâ‚‚ : ValuesH M) (uge : âˆ€h : Hist M, uâ‚ h â‰¥ uâ‚‚ h) :
        âˆ€h : Hist M, âˆ€Ï€ : PolicyHR M, DPhopt uâ‚ h â‰¥ DPhÏ€ Ï€ uâ‚‚ h := sorry


theorem dph_correct_vf (Ï€ : PolicyHR M) (t : â„•) (h : Hist M) : 
                      hvalue_Ï€ Ï€ t h = u_dp_Ï€ Ï€ t h := 
   match t with
     | Nat.zero => rfl
     | Nat.succ t => 
       let hp h' := dph_correct_vf Ï€ t h'
       sorry

theorem dph_opt_vf_opt (t : â„•) : 
        âˆ€Ï€ : PolicyHR M, âˆ€ h : Hist M, u_dp_opt t h â‰¥ u_dp_Ï€ Ï€ t h := sorry 

end DPValueH
