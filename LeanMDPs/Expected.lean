import LeanMDPs.Histories

--set_option diagnostics true
--set_option diagnostics.threshold 3

open NNReal

/- state -/
variable {Ïƒ : Type} [Inhabited Ïƒ] [DecidableEq Ïƒ] 
/- action -/
variable {Î± : Type} [Inhabited Î±] [DecidableEq Î±]
variable {M : MDP Ïƒ Î±}

open Finprob

section Objectives

-- Future generalization??
/- Generic objective definition -/
--def Objective (Ïƒ : Type) (Î± : Type) := MDP Ïƒ Î± â†’ Type
/- General definition of an objective function -/
--class ObjectiveFun (o : Objective Ïƒ Î±) where
--  obj : Phr M â†’ â„

/-- Finite horizon objective parameters -/
structure ObjectiveFH (M : MDP Ïƒ Î±) : Type where
  sâ‚€ : M.S -- initial state
  T : â„• -- horizon


/-- Objective function -/
def objective_fh (O : ObjectiveFH M) (Ï€ : Phr M) := ð”¼â‚•[ reward // O.sâ‚€, Ï€, O.T]

/-- An optimal policy Ï€opt -/
def Optimal_fh (O : ObjectiveFH M) (Ï€opt : Phr M) := âˆ€ Ï€ : Phr M, objective_fh O Ï€opt â‰¥ objective_fh O Ï€

/-- Value function type for value functions that are -/
def ValuesH (M : MDP Ïƒ Î±) : Type := Hist M â†’ â„

/-- History-dependent value function -/
def hvalue_Ï€ (Ï€ : PolicyHR M) : â„• â†’ ValuesH M
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => fun h â†¦ ð”¼â‚•[ reward // h, Ï€, t.succ ] 
  

-- TODO: This needs some thought to be defined properly
--def hvalue_opt : â„• â†’ ValuesH M
--    | Nat.zero => fun _ â†¦ 0
--    | Nat.succ t => fun h => fun Ï€ â†¦ hvalue_Ï€ Ï€  


/-- An value-optimal policy Ï€opt -/
def OptimalPi_fh (t : â„•) (Ï€opt : Phr M):= âˆ€ Ï€ : Phr M, âˆ€ h : Hist M, hvalue_Ï€ Ï€opt t h â‰¥ hvalue_Ï€ Ï€ t h

end Objectives

section DPValueH

/-- Bellman operator on history-dependent value functions -/
def DPhÏ€ (Ï€ : PolicyHR M) (vâ‚œ : ValuesH M) : ValuesH M 
  | h => âˆ‘ a âˆˆ M.A, âˆ‘ s' âˆˆ M.S, ((Ï€ h).p a * (M.P h.last a).p s') * (M.r h.last a s' + vâ‚œ h)

/-- Bellman operator on history-dependent value functions -/
def DPhopt (vâ‚œ : ValuesH M) : ValuesH M 
  | h => let q a :=  âˆ‘ s' âˆˆ M.S, (M.P h.last a).p s' * (M.r h.last a s' + vâ‚œ h)
         M.A.sup' M.A_ne q

/-- Dynamic program value function, finite-horizon history dependent -/
def u_dp_Ï€ (Ï€ : PolicyHR M) : â„• â†’ ValuesH M 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPhÏ€ Ï€ (u_dp_Ï€ Ï€ t)


/-- Dynamic program value function, finite-horizon history dependent -/
def u_dp_opt  : â„• â†’ ValuesH M 
  | Nat.zero => fun _ â†¦ 0
  | Nat.succ t => DPhopt (u_dp_opt t)

theorem dp_correct_vf (Ï€ : PolicyHR M) (T : â„•) (h : Hist M) : 
                      hvalue_Ï€ Ï€ T h = u_dp_Ï€ Ï€ T h := 
   match T with
     | Nat.zero => rfl
     | Nat.succ t => 
       let hp h' := dp_correct_vf Ï€ t h'
       sorry

end DPValueH
