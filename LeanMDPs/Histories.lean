/-
In this file we define histories and operations that are related to them. 

* Defines an MDP
* Defines a history, which is a sequence of states and actions
* Defines a histories consistent with a partial sequence of states and actions
* A general randomized history-dependent policy
* The reward and probability of the history, which is used to compute the value function
* Value function for a history as the expected reward
-/

import Mathlib.Data.Nat.Defs

import Mathlib.Data.Real.Basic 
import Mathlib.Data.NNReal.Basic

--import Mathlib.Data.Set.Basic
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Finset.Image
import Mathlib.Logic.Function.Defs -- Injective

import Mathlib.Probability.ProbabilityMassFunction.Basic
--variable (Î± Ïƒ : Type)

import LeanMDPs.Finprob

variable {Ïƒ Î± : Type}
--variable [Inhabited Ïƒ] [Inhabited Î±] -- used to construct policies

open NNReal -- for â„â‰¥0 notation
open Finprob

section Definitions

/-- Markov decision process -/
structure MDP (Ïƒ Î± : Type) : Type where
  /-- states , TODO: consider ğ’® or ğ“¢ but causes issues-/
  S : Finset Ïƒ
  /-- actions, TODO: consider ğ’œ or ğ“ but causes issues  -/
  A : Finset Î±
  /-- transition probability s, a, s' -/
  P : Ïƒ â†’ Î± â†’ Î” S  -- TODO : change to S â†’ A â†’ Î” S
  /-- reward function s, a, s' -/
  r : Ïƒ â†’ Î± â†’ Ïƒ â†’ â„ -- TODO: change to S â†’ A â†’ S â†’ â„

variable {M : MDP Ïƒ Î±}

/-- Represents a history. The state is type Ïƒ and action is type Î±. -/
inductive Hist {Ïƒ Î± : Type} (M : MDP Ïƒ Î±)  : Type where
  | init : Ïƒ â†’ Hist M
  | prev : Hist M â†’ Î± â†’ Ïƒ â†’ Hist M

/-- The length of the history corresponds to the zero-based step of the decision -/
def Hist.length : Hist M â†’ â„•
  | init _ => 0
  | prev h _ _ => 1 + length h 


/-- Nonempty histories -/
abbrev HistNE {Ïƒ Î± : Type} (m : MDP Ïƒ Î±) : Type := {h : Hist m // h.length â‰¥ 1}

/-- Returns the last state of the history -/
def Hist.last : Hist M â†’ Ïƒ
  | init s => s
  | prev _ _ s => s

/-- Appends the state and action to the history --/
def Hist.append (h : Hist M) (as : Î± Ã— Ïƒ) : Hist M :=
  Hist.prev h as.fst as.snd
  
def tuple2hist : Hist M Ã— Î± Ã— Ïƒ â†’ HistNE M
  | âŸ¨h, asâŸ© => âŸ¨h.append as, Nat.le.intro rflâŸ©

def hist2tuple : HistNE M â†’ Hist M Ã— Î± Ã— Ïƒ
  | âŸ¨Hist.prev h a s, _ âŸ© => âŸ¨h, a, sâŸ©

/-- Proves that history append has a left inverse. -/
lemma linv_hist2tuple_tuple2hist : 
      Function.LeftInverse (hist2tuple (M := M)) tuple2hist := fun _ => rfl

lemma inj_tuple2hist_l1 : Function.Injective (tuple2hist (M:=M)) :=
            Function.LeftInverse.injective linv_hist2tuple_tuple2hist

lemma inj_tuple2hist :  
  Function.Injective ((Subtype.val) âˆ˜ (tuple2hist (M:=M))) := 
    Function.Injective.comp (Subtype.val_injective) inj_tuple2hist_l1

/-- New history from a tuple. -/
def emb_tuple2hist_l1 : Hist M Ã— Î± Ã— Ïƒ â†ª HistNE M :=
 { toFun := tuple2hist, inj' := inj_tuple2hist_l1 }
 
def emb_tuple2hist : Hist M Ã— Î± Ã— Ïƒ â†ª Hist M  :=
 { toFun := fun x => tuple2hist x, inj' := inj_tuple2hist }

/-- Checks if pre is the prefix of h. -/
def isprefix : Hist M â†’ Hist M â†’ Prop 
    | Hist.init sâ‚, Hist.init sâ‚‚ => sâ‚ = sâ‚‚
    | Hist.init sâ‚, Hist.prev hp _ _ => isprefix (Hist.init sâ‚) hp 
    | Hist.prev _ _ _, Hist.init _ => False
    | Hist.prev hâ‚ aâ‚ sâ‚', Hist.prev  hâ‚‚ aâ‚‚ sâ‚‚' => 
        if hâ‚.length > hâ‚‚.length then
            False
        else if hâ‚.length < hâ‚‚.length then
            let pre := Hist.prev hâ‚ aâ‚ sâ‚' 
            isprefix pre hâ‚‚
        else
            (aâ‚ = aâ‚‚) âˆ§ (sâ‚' = sâ‚‚') âˆ§ (isprefix hâ‚ hâ‚‚)

/-- Decision rule -/
def DecisionRule (M : MDP Ïƒ Î±) := Ïƒ â†’ M.A

/-- A randomized history-dependent policy -/
def PolicyHR (M : MDP Ïƒ Î±) : Type := Hist M â†’ Î” M.A
abbrev Phr : MDP Ïƒ Î± â†’ Type := PolicyHR

/-- A deterministic Markov policy -/
def PolicyMD (M : MDP Ïƒ Î±) : Type := â„• â†’ Ïƒ â†’ M.A 
abbrev Pmd : MDP Ïƒ Î± â†’ Type := PolicyMD

/-- A deterministic stationary policy -/
def PolicySD (M : MDP Ïƒ Î±) : Type := Ïƒ â†’ M.A
abbrev Psd : MDP Ïƒ Î± â†’ Type := PolicySD

instance [DecidableEq Î±] : Coe (PolicySD M) (PolicyHR M) where
  coe d := fun h â†¦ dirac_dist M.A (d h.last)

instance [DecidableEq Î±] : Coe (PolicyMD M) (PolicyHR M) where
  coe d := fun h â†¦ dirac_dist M.A (d h.length h.last)

/-- Set of all histories of additional length T that follow history `h`. -/
def Histories (h : Hist M) : â„• â†’ Finset (Hist M) 
    | Nat.zero => {h}
    | Nat.succ t => ((Histories h t) Ã—Ë¢ M.A Ã—Ë¢ M.S).map emb_tuple2hist

abbrev â„‹ : Hist M â†’ â„• â†’ Finset (Hist M) := Histories

/-- Probability distribution over histories induced by the policy and 
    transition probabilities -/
def HistDist (h : Hist M) (Ï€ : PolicyHR M) (T : â„•) : Î” (â„‹ h T) :=
  match T with 
    | Nat.zero => dirac_ofsingleton h
    | Nat.succ t => 
      let prev := HistDist h Ï€ t -- previous history
      -- probability of the history
      let f h' (as : Î± Ã— Ïƒ) := ((Ï€ h').p as.1 * (M.P h'.last as.1).p as.2)
      -- the second parameter below is the proof of being in Phist pre t; not used
      let sumsto_as (h' : Hist M) _ : âˆ‘ as âˆˆ M.A Ã—Ë¢ M.S, f h' as = 1 :=
          prob_prod_prob (Ï€ h').p (fun a =>(M.P h'.last a).p ) 
                         (Ï€ h').sumsto (fun a _ => (M.P h'.last a).sumsto)
      let sumsto : âˆ‘ âŸ¨h',asâŸ© âˆˆ ((Histories h t) Ã—Ë¢ M.A Ã—Ë¢ M.S), prev.p h' * f h' as = 1 := 
          prob_prod_prob prev.p f prev.sumsto sumsto_as 
      let HAS := ((Histories h t) Ã—Ë¢ M.A Ã—Ë¢ M.S).map emb_tuple2hist
      let p : Hist M â†’ â„â‰¥0 
        | Hist.init _ => 0 --ignored
        | Hist.prev h' a s => prev.p h' * f h' âŸ¨a,sâŸ©
      have sumsto_fin : âˆ‘ h' âˆˆ HAS, p h'  = 1 := 
          (Finset.sum_map ((Histories h t) Ã—Ë¢ M.A Ã—Ë¢ M.S) emb_tuple2hist p) â–¸ sumsto
      âŸ¨p, sumsto_finâŸ©

abbrev Î”â„‹ (h : Hist M) (Ï€ : PolicyHR M) (T : â„•) : Finprob (Hist M) := 
          âŸ¨â„‹ h T, HistDist h Ï€ TâŸ©

/- Computes the probability of a history -/
/-def probability  (Ï€ : PolicyHR m) : Hist m â†’ â„â‰¥0 
      | Hist.init s => m.Î¼.p s
      | Hist.prev hp a s' => probability Ï€ hp * ((Ï€ hp).p a * (m.P hp.last a).p s')  
-/

/-- Reward of a history -/
def reward : Hist M â†’ â„ 
    | Hist.init _ => 0
    | Hist.prev hp a s' => (M.r hp.last a s') + (reward hp)  

/-- The probability of a history -/
def prob_h (h : Hist M) (Ï€ : PolicyHR M) (T : â„•) (h' : â„‹ h T) : â„â‰¥0 := (Î”â„‹ h Ï€ T).2.p h'

--theorem hdist_eq_prod_pi_P  (h : Hist M) (Ï€ : Î ) 

/- ----------- Expectations ---------------- -/


--variable {Ï : Type} [HMulZero Ï] [HMul â„• Ï Ï] [AddCommMonoid Ï]

/-- Expectation over histories for a r.v. X for horizon T and policy Ï€ -/
def expect_h (h : Hist M) (Ï€ : PolicyHR M) (T : â„•) (X : Hist M â†’ â„) : â„ := 
        have P := Î”â„‹ h Ï€ T
        expect (âŸ¨XâŸ© : Finrv P â„)

notation "ğ”¼â‚•[" X "//" h "," Ï€ "," T "]" => expect_h h Ï€ T X

/- Conditional expectation with future singletons -/
/-theorem hist_tower_property {hâ‚– : Hist m} {Ï€ : PolicyHR m} {t : â„•} {f : Hist m â†’ â„}
  (valid : hâ‚– âˆˆ â„‹ hâ‚– t) 
  : ğ”¼â‚• hâ‚– Ï€ 1 f = (âˆ‘ a âˆˆ m.A, âˆ‘ s âˆˆ m.S, f (Hist.prev hâ‚– a s)) := sorry
-/
-- TODO: write a general tower property result first, and then derive a version of this
-- result, which needs to apply over multiple time steps. 

/-
TODO:
2. Show that is the policy is Markov then also the value function is Markov
-/
