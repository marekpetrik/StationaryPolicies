import Mathlib.Data.Nat.Defs

import Mathlib.Data.Real.Basic 
import Mathlib.Data.NNReal.Basic

--import Mathlib.Data.Set.Basic
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Finset.Image
import Mathlib.Logic.Function.Defs -- Injective

import Mathlib.Probability.ProbabilityMassFunction.Basic
--variable (Œ± œÉ : Type)

import LeanMDPs.FinPr

/-
In this file we define histories and operations that are related to them. 

* Defines an MDP
* Defines a history, which is a sequence of states and actions
* Defines a histories consistent with a partial sequence of states and actions
* A general randomized history-dependent policy
* The reward and probability of the history, which is used to compute the value function
* Value function for a history as the expected reward
-/

variable {œÉ Œ± : Type}
variable [Inhabited œÉ] [Inhabited Œ±] -- used to construct policies

open NNReal -- for ‚Ñù‚â•0 notation
open FinP

/--
The Markov decision process definition 

TODO: Consider defining P and r only of the subtypes constructed from 
the Finsets S and A
-/
structure MDP (œÉ Œ± : Type) : Type where
  /-- states , TODO: consider ùíÆ or ùì¢ but causes issues-/
  S : Finset œÉ
  /-- actions, TODO: consider ùíú or ùìê but causes issues  -/
  A : Finset Œ±
  /-- transition probability s, a, s' -/
  P : œÉ ‚Üí Œ± ‚Üí Œî S  -- TODO : change to S ‚Üí A ‚Üí Œî S
  /-- reward function s, a, s' -/
  r : œÉ ‚Üí Œ± ‚Üí œÉ ‚Üí ‚Ñù -- TODO: change to S ‚Üí A ‚Üí S ‚Üí ‚Ñù
  /-- initial distribution -/
  Œº : Œî S

variable {m : MDP œÉ Œ±}

/--
Represents a history. The state is type œÉ and action is type Œ±.

The representation of the history is backwards to facilitate the 
application of a policy
-/
inductive Hist {œÉ Œ± : Type} (m : MDP œÉ Œ±)  : Type where
  | init : œÉ ‚Üí Hist m
  | prev : Hist m ‚Üí Œ± ‚Üí œÉ ‚Üí Hist m

/--
The length of the history corresponds to the zero-based step of the decision
-/
def Hist.length : Hist m ‚Üí ‚Ñï
  | init _ => 0
  | prev h _ _ => HAdd.hAdd (length h) 1

/-- returns the last state of the history -/
def Hist.last  : Hist m ‚Üí œÉ
  | init s => s
  | prev _ _ s => s

/-- appends the state and action to the history --/
def Hist.append (h : Hist m) (as : Œ± √ó œÉ) : Hist m :=
  Hist.prev h as.fst as.snd
  
/--
Creates new histories from combinations of shorter histories
and states and actions.
-/
def tuple2hist : Hist m √ó Œ± √ó œÉ ‚Üí Hist m
  | ‚ü®h, as‚ü© => h.append as

def hist2tuple : Hist m ‚Üí Hist m √ó Œ± √ó œÉ
  | Hist.prev h a s => ‚ü® h, a, s ‚ü©
  -- the second case is not used
  | Hist.init s => ‚ü® (Hist.init s), default, default ‚ü©

/-- Proves that history append has a left inverse. This is used 
    to show that the tupple2hist is an embedding, useful when 
    constructing a set of histories -/
lemma linv_hist2tuple_tuple2hist : 
         Function.LeftInverse (hist2tuple (m := m) ) tuple2hist := fun _ => rfl

/--
Creates new histories from combinations of shorter histories
and states and actions. The embedding guarantees it is injective
-/
def emb_tuple2hist : Hist m √ó Œ± √ó œÉ ‚Ü™ Hist m :=
 { toFun := tuple2hist, inj' := Function.LeftInverse.injective linv_hist2tuple_tuple2hist }

/-- Checks if pre is the prefix of h. -/
def isprefix : Hist m ‚Üí  Hist m ‚Üí Prop 
    | Hist.init s‚ÇÅ, Hist.init s‚ÇÇ => s‚ÇÅ = s‚ÇÇ
    | Hist.init s‚ÇÅ, Hist.prev hp _ _ => isprefix (Hist.init s‚ÇÅ) hp 
    | Hist.prev _ _ _, Hist.init _ => False
    | Hist.prev h‚ÇÅ a‚ÇÅ s‚ÇÅ', Hist.prev  h‚ÇÇ a‚ÇÇ s‚ÇÇ' => 
        if h‚ÇÅ.length > h‚ÇÇ.length then
            False
        else if h‚ÇÅ.length < h‚ÇÇ.length then
            let pre := Hist.prev h‚ÇÅ a‚ÇÅ s‚ÇÅ' 
            isprefix pre h‚ÇÇ
        else
            (a‚ÇÅ = a‚ÇÇ) ‚àß (s‚ÇÅ' = s‚ÇÇ') ‚àß (isprefix h‚ÇÅ h‚ÇÇ)

/-- A randomized history-dependent policy -/
def PolicyHR (m : MDP œÉ Œ±) := Hist m ‚Üí Œî m.A

/-- 
Set of all histories of additional length T that follow history `h`.
-/
def Histories (h : Hist m) : ‚Ñï ‚Üí Finset (Hist m) 
    | Nat.zero => {h}
    | Nat.succ t =>  ((Histories h t) √óÀ¢ m.A √óÀ¢ m.S).map emb_tuple2hist 

abbrev ‚Ñã : Hist m ‚Üí ‚Ñï ‚Üí Finset (Hist m) := Histories


def HistDist (h‚Çñ : Hist m) (œÄ : PolicyHR m) (T : ‚Ñï) : Œî (‚Ñã h‚Çñ T) :=
  match T with 
    | Nat.zero => dirac_ofsingleton h‚Çñ
    | Nat.succ t => 
      let prev := HistDist h‚Çñ œÄ t -- previous history
      -- probability of the history
      let f h (as : Œ± √ó œÉ) := ((œÄ h).p as.1 * (m.P h.last as.1).p as.2)
      -- the second parameter below is the proof of being in Phist pre t; not used
      let sumsto_as (h' : Hist m) _ : ‚àë as ‚àà m.A √óÀ¢ m.S, f h' as = 1 :=
          prob_prod_prob (œÄ h').p (fun a =>(m.P h'.last a).p ) 
                         (œÄ h').sumsto (fun a _ => (m.P h'.last a).sumsto)
      let sumsto : ‚àë ‚ü®h,as‚ü© ‚àà ((Histories h‚Çñ t) √óÀ¢ m.A √óÀ¢ m.S), prev.p h * f h as = 1 := 
          prob_prod_prob prev.p f prev.sumsto sumsto_as 
      let HAS := ((Histories h‚Çñ t) √óÀ¢ m.A √óÀ¢ m.S).map emb_tuple2hist
      let p : Hist m ‚Üí ‚Ñù‚â•0 
        | Hist.init _ => 0
        | Hist.prev h' a s => prev.p h' * f h' ‚ü®a,s‚ü©
      let sumsto_fin : ‚àë h ‚àà HAS, p h  = 1 := 
          (Finset.sum_map ((Histories h‚Çñ t) √óÀ¢ m.A √óÀ¢ m.S) emb_tuple2hist p) ‚ñ∏ sumsto
      {p := p, sumsto := sumsto_fin}

abbrev Œî‚Ñã : (h : Hist m) ‚Üí PolicyHR m ‚Üí (T : ‚Ñï) ‚Üí Œî (‚Ñã h T) := HistDist

/-- Computes the probability of a history -/
def probability (œÄ : PolicyHR m) : Hist m ‚Üí ‚Ñù‚â•0 
      | Hist.init s => m.Œº.p s
      | Hist.prev hp a s' => probability œÄ hp * ((œÄ hp).p a * (m.P hp.last a).p s')  
 
def probability_pre  (œÄ : PolicyHR m) : Hist m ‚Üí ‚Ñù‚â•0 
      | Hist.init s => m.Œº.p s
      | Hist.prev hp a s' => probability œÄ hp * ((œÄ hp).p a * (m.P hp.last a).p s')  

/-- Compute the probability of a history 
-/
def ‚Ñô‚Çï (pre : Hist m) (œÄ : PolicyHR m) (T : ‚Ñï)  : FinP (Histories pre T) := sorry

/--
Computes the reward of a history
-/
def reward : Hist m ‚Üí ‚Ñù 
    | Hist.init _ => 0.
    | Hist.prev hp a s' => (m.r hp.last a s') + (reward hp)  



/-
TODO:
2. Show that is the policy is Markov then also the value function is Markov
-/
