import Mathlib.Data.Nat.Defs
import Mathlib.Data.Real.Basic 
import Mathlib.Data.NNReal.Basic

--import Mathlib.Data.Set.Basic
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Finset.Image
import Mathlib.Logic.Function.Defs -- Injective

import Mathlib.Probability.ProbabilityMassFunction.Basic
--variable (Î± Ïƒ : Type)

/-
In this file we define histories and operations that are related to them. 

* Defines an MDP
* Defines a history, which is a sequence of states and actions
* Defines a histories consistent with a partial sequence of states and actions
* A general randomized history-dependent policy
* The reward and probability of the history, which is used to compute the value function
* Value function for a history as the expected reward

-/

#check PMF
#check Finset
#check Sigma 
#check Multiset.sum
#check Set

#eval 1 âˆˆ [1,2,3] 
#check Membership.mem [1,2,3] 1
 
#check Inhabited
#check Embedding

variable {Ïƒ Î± : Type}
variable [Inhabited Ïƒ] [Inhabited Î±]
variable [DecidableEq Ïƒ] [DecidableEq Î±]

/--
Represents a history. The state is type Ïƒ and action is type Î±.

The representation of the history is backwards to facilitate the 
application of a policy
-/
inductive Hist (Ïƒ Î± : Type)  : Type where
  | init : Ïƒ â†’ Hist Ïƒ Î±
  | prev : Hist Ïƒ Î± â†’ Î± â†’ Ïƒ â†’ Hist Ïƒ Î±  

/--
The length of the history corresponds to the zero-based step of the decision
-/
def Hist.length : Hist Ïƒ Î± â†’ â„•
  | init _ => 0
  | prev h _ _ => HAdd.hAdd (length h) 1

/-- returns the last state of the history -/
def Hist.laststate  : Hist Ïƒ Î± â†’ Ïƒ
  | init s => s
  | prev _ _ s => s

/-- appends the state and action to the history --/
def Hist.append {Ïƒ Î± : Type} (h : Hist Ïƒ Î±) (as : Î± Ã— Ïƒ) : Hist Ïƒ Î± :=
  Hist.prev h as.fst as.snd
  
/--
Creates new histories from combinations of shorter histories
and states and actions.
-/
def append_hist : Hist Ïƒ Î± Ã— Î± Ã— Ïƒ â†’ Hist Ïƒ Î± 
  | âŸ¨h, asâŸ© => h.append as

def append_hist_linv : Hist Ïƒ Î± â†’ Hist Ïƒ Î± Ã— Î± Ã— Ïƒ
  | Hist.prev h a s => âŸ¨ h, a, s âŸ©
  -- the second case is not used
  | Hist.init _ => âŸ¨ (Hist.init default), default, default âŸ©

/-- Proves that history append has a left inverse. This is used 
    to show that the append_hist is an embedding, useful when 
    constructing a set of histories -/
lemma linv_append_hist {Ïƒ Î± : Type}  [Inhabited Ïƒ] [Inhabited Î±] : 
    Function.LeftInverse (append_hist_linv (Ïƒ := Ïƒ) (Î± := Î±)) append_hist := fun _ => rfl

/--
checks if pre is the prefix of h. This is needed when defining value functions
-/
def isprefix [DecidableEq Ïƒ] [DecidableEq Î±] (pre : Hist Ïƒ Î±) (h : Hist Ïƒ Î±) : Prop :=
  match pre, h with
    | Hist.init sâ‚, Hist.init sâ‚‚ => sâ‚ = sâ‚‚
    | Hist.init _, Hist.prev hp _ _ => isprefix pre hp 
    | Hist.prev _ _ _, Hist.init _ => False
    | Hist.prev hâ‚ aâ‚ sâ‚', Hist.prev  hâ‚‚ aâ‚‚ sâ‚‚' => 
        if hâ‚.length > hâ‚‚.length then
            False
        else if hâ‚.length < hâ‚‚.length then
            isprefix pre hâ‚‚
        else
            (aâ‚ = aâ‚‚) âˆ§ (sâ‚' = sâ‚‚') âˆ§ (isprefix hâ‚ hâ‚‚)

open NNReal -- for â„â‰¥0 notation

/--
The Markov decision process definition 

TODO: Consider defining P and r only of the subtypes constructed from 
the Finsets ğ’® and ğ’œ
-/
structure MDP (Ïƒ Î± : Type) : Type where
  /-- states , TODO: consider \McS but causes issues-/
  SS : Finset Ïƒ
  /-- actions, TODO: consider \McA but causes issues  -/
  AA : Finset Î±
  /-- transition probability s, a, s' -/
  -- TODO: should be a probability distribution
  P  : Ïƒ â†’ Î± â†’ Ïƒ â†’ â„â‰¥0
  /-- proof of transition probability --/
  prob : (s : Ïƒ) â†’ (a : Î±) â†’ (âˆ‘ s' âˆˆ SS, P s a s') = 1
  /-- reward function s, a, s' -/
  r  : Ïƒ â†’ Î± â†’ Ïƒ â†’ â„
  /-- initial state -/
  sâ‚€ : Ïƒ
  -- TODO: all these functions need to be only defined for states and actions
  -- should be using a Subtype {s // s âˆˆ states} and Finset attach?

/--
A general randomized history-dependent policy
-/
structure Policy (m : MDP Ïƒ Î±) : Type where
  Ï€ : Hist Ïƒ Î± â†’ Î± â†’ â„â‰¥0
  /-- proof that it sums to 1 for all states -/
  prob : (h : Hist Ïƒ Î±) â†’ (âˆ‘ a âˆˆ m.AA, Ï€ h a) = 1

/-- The set of all histories of length T -/
def HistAll {Ïƒ Î± : Type} (T : â„•) := { h : Hist Ïƒ Î± | h.length = T }


-- Need to prove the function used in the construction is injective

 
--lemma append_hist_inj : Function.Injective (append_hist (Ïƒ := Ïƒ) (Î± := Î±)) :=
--   fun aâ‚ aâ‚‚ eq => Eq.rec (fun a b => rfl) eq
     

/--
Creates new histories from combinations of shorter histories
and states and actions.

The embedding guarantees it is injective
-/
def emb_hist_as : Hist Ïƒ Î± Ã— Î± Ã— Ïƒ â†ª Hist Ïƒ Î± :=
 { toFun := append_hist, inj' := Function.LeftInverse.injective linv_append_hist }

/-- 
Set of all policies that follow history pre.
Note that this is just a definition of the set and not a specific instance of the set

The function allhist constructs all histories that satisfy the condition of this set
-/
def PHist  [DecidableEq Ïƒ] [DecidableEq Î±] 
          (m : MDP Ïƒ Î±) (pre : Hist Ïƒ Î±) (T : â„•) : Finset (Hist Ïƒ Î±)  := 
             if T < pre.length then
               Finset.empty
             else if T = pre.length then
               {pre}
             else
               let AS := Finset.product m.AA  m.SS
               let HAS := Finset.product (PHist m pre (T-1)) AS
               Finset.map emb_hist_as HAS 
               
/--
Computes the probability of a history
-/
noncomputable def probability  [DecidableEq Ïƒ] (m : MDP Ïƒ Î±) 
                              (Ï€ : Policy m) : Hist Ïƒ Î± â†’ â„â‰¥0 
      | Hist.init s => if m.sâ‚€ = s then 1. else 0.
      | Hist.prev hp a s' => (m.P hp.laststate a s') * (Ï€.Ï€ hp a) * (probability m Ï€ hp)  

/--
Computes the reward of a history
-/
noncomputable def reward  (m : MDP Ïƒ Î±) :  Hist Ïƒ Î± â†’ â„ 
    | Hist.init _ => 0.
    | Hist.prev hp a s'  =>  (m.r hp.laststate a s') + (reward m hp)  


/-- show that history probabilities are actually a probability distribution -/
lemma probability_dist (m : MDP Ïƒ Î±) (pre : Hist Ïƒ Î±) (Ï€ : Policy m) (T : â„•) : 
                       (âˆ‘ h âˆˆ PHist m pre T, (fun h => probability m Ï€) h) = 1 := sorry


#check Finset.sum


/-

TODO:

1. Dynamic program for histories
2. Show that is the policy is Markov then also the value function is Markov

-/
