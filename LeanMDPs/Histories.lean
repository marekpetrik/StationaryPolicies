import Mathlib.Data.Nat.Defs

import Mathlib.Data.Real.Basic 
import Mathlib.Data.NNReal.Basic

--import Mathlib.Data.Set.Basic
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Finset.Image
import Mathlib.Logic.Function.Defs -- Injective

import Mathlib.Probability.ProbabilityMassFunction.Basic
--variable (Î± Ïƒ : Type)

/-
In this file we define histories and operations that are related to them. 

* Defines an MDP
* Defines a history, which is a sequence of states and actions
* Defines a histories consistent with a partial sequence of states and actions
* A general randomized history-dependent policy
* The reward and probability of the history, which is used to compute the value function
* Value function for a history as the expected reward
-/

variable {Ïƒ Î± : Type}
variable [Inhabited Ïƒ] [Inhabited Î±]

open NNReal -- for â„â‰¥0 notation

/--
The Markov decision process definition 

TODO: Consider defining P and r only of the subtypes constructed from 
the Finsets S and A
-/
structure MDP (Ïƒ Î± : Type) : Type where
  /-- states , TODO: consider ğ’® or ğ“¢ but causes issues-/
  S : Finset Ïƒ
  /-- actions, TODO: consider ğ’œ or ğ“ but causes issues  -/
  A : Finset Î±
  /-- transition probability s, a, s' -/
  -- TODO: should be a probability distribution
  P : Ïƒ â†’ Î± â†’ Ïƒ â†’ â„â‰¥0
  /-- proof of transition probability --/
  prob : (s : Ïƒ) â†’ (a : Î±) â†’ (âˆ‘ s' âˆˆ S, P s a s') = 1
  /-- reward function s, a, s' -/
  r : Ïƒ â†’ Î± â†’ Ïƒ â†’ â„
  /-- initial state -/
  sâ‚€ : Ïƒ
  sâ‚€inS : (sâ‚€ âˆˆ S)
  -- TODO: all these functions need to be only defined for states and actions
  -- should be using a Subtype {s // s âˆˆ states} and Finset attach?

variable {m : MDP Ïƒ Î±}

/--
Represents a history. The state is type Ïƒ and action is type Î±.

The representation of the history is backwards to facilitate the 
application of a policy
-/
inductive Hist {Ïƒ Î± : Type} (m : MDP Ïƒ Î±)  : Type where
  | init : Ïƒ â†’ Hist m
  | prev : Hist m â†’ Î± â†’ Ïƒ â†’ Hist m

/--
The length of the history corresponds to the zero-based step of the decision
-/
def Hist.length : Hist m â†’ â„•
  | init _ => 0
  | prev h _ _ => HAdd.hAdd (length h) 1

/-- returns the last state of the history -/
def Hist.last  : Hist m â†’ Ïƒ
  | init s => s
  | prev _ _ s => s

/-- appends the state and action to the history --/
def Hist.append (h : Hist m) (as : Î± Ã— Ïƒ) : Hist m :=
  Hist.prev h as.fst as.snd
  
/--
Creates new histories from combinations of shorter histories
and states and actions.
-/
def tuple2hist : Hist m Ã— Î± Ã— Ïƒ â†’ Hist m
  | âŸ¨h, asâŸ© => h.append as

def hist2tuple : Hist m â†’ Hist m Ã— Î± Ã— Ïƒ
  | Hist.prev h a s => âŸ¨ h, a, s âŸ©
  -- the second case is not used
  | Hist.init _ => âŸ¨ (Hist.init default), default, default âŸ©

/-- Proves that history append has a left inverse. This is used 
    to show that the tupple2hist is an embedding, useful when 
    constructing a set of histories -/
lemma linv_hist2tuple_tuple2hist : Function.LeftInverse (hist2tuple (m := m) ) tuple2hist := fun _ => rfl

/--
Checks if pre is the prefix of h. This is needed when defining value functions
-/
def isprefix (pre : Hist m) (h : Hist m) : Prop :=
  match pre, h with
    | Hist.init sâ‚, Hist.init sâ‚‚ => sâ‚ = sâ‚‚
    | Hist.init _, Hist.prev hp _ _ => isprefix pre hp 
    | Hist.prev _ _ _, Hist.init _ => False
    | Hist.prev hâ‚ aâ‚ sâ‚', Hist.prev  hâ‚‚ aâ‚‚ sâ‚‚' => 
        if hâ‚.length > hâ‚‚.length then
            False
        else if hâ‚.length < hâ‚‚.length then
            isprefix pre hâ‚‚
        else
            (aâ‚ = aâ‚‚) âˆ§ (sâ‚' = sâ‚‚') âˆ§ (isprefix hâ‚ hâ‚‚)

/--
A general randomized history-dependent policy
-/
structure Policy (m : MDP Ïƒ Î±) : Type where
  Ï€ : Hist m â†’ Î± â†’ â„â‰¥0
  /-- proof that it sums to 1 for all states -/
  prob : (h : Hist m) â†’ (âˆ‘ a âˆˆ m.A, Ï€ h a) = 1

/--
Creates new histories from combinations of shorter histories
and states and actions.

The embedding guarantees it is injective
-/
def emb_tuple2hist : Hist m Ã— Î± Ã— Ïƒ â†ª Hist m :=
 { toFun := tuple2hist, inj' := Function.LeftInverse.injective linv_hist2tuple_tuple2hist }

/-- 
Set of all policies that follow history pre.
Note that this is just a definition of the set and not a specific instance of the set

T is the number of steps beyond the history pre
-/
def PHist (pre : Hist m) (T : â„•) : Finset (Hist m) := 
    match T with 
      | Nat.zero => {pre}
      | Nat.succ t => Finset.map emb_tuple2hist ((PHist pre t) Ã—Ë¢ m.A Ã—Ë¢ m.S)

/--
Computes the probability of a history
-/
noncomputable def probability [DecidableEq Ïƒ] (Ï€ : Policy m) : Hist m â†’ â„â‰¥0 
      | Hist.init s => if m.sâ‚€ = s then 1. else 0.
      | Hist.prev hp a s' => probability Ï€ hp * (Ï€.Ï€ hp a * m.P hp.last a s')  
 
noncomputable def probability_has [DecidableEq Ïƒ] (Ï€ : Policy m) : Hist m Ã— Î± Ã— Ïƒ â†’ â„â‰¥0 
      | âŸ¨h,a,sâŸ© => probability Ï€ h * (Ï€.Ï€ h a * m.P h.last a s)

lemma hist_prob (Ï€ : Policy m) [DecidableEq Ïƒ]: 
       âˆ€ has, probability Ï€ (emb_tuple2hist has) = probability_has Ï€ has := fun _ => rfl
/--
Computes the reward of a history
-/
noncomputable def reward : Hist m â†’ â„ 
    | Hist.init _ => 0.
    | Hist.prev hp a s' => (m.r hp.last a s') + (reward hp)  

--example {Sâ‚ Sâ‚‚ : Finset Ïƒ} (sâ‚ : Ïƒ) (f : â„ ) (g : Ïƒ â†’ â„) : f*(âˆ‘ sâ‚‚ âˆˆ Sâ‚‚, (g sâ‚‚)) = âˆ‘ sâ‚‚ âˆˆ Sâ‚‚, f*(g sâ‚‚) := by apply Finset.mul_sum

#check Finset.product

lemma prob_prod {A : Finset Î±} {S : Finset Ïƒ} (f : Î± â†’ â„â‰¥0) (g : Î± â†’ Ïƒ â†’ â„â‰¥0) 
                 (h1 : âˆ€ a : Î±, âˆ‘ s âˆˆ S, g a s = 1) (h2 : âˆ‘ a âˆˆ A, f a = 1): 
          (âˆ‘ âŸ¨a,sâŸ© âˆˆ (A Ã—Ë¢ S), (f a) * (g a s) ) = 1  := 
          calc 
          âˆ‘ âŸ¨a,sâŸ© âˆˆ (A Ã—Ë¢ S), (f a)*(g a s)  = âˆ‘ a âˆˆ A, âˆ‘ sâ‚‚ âˆˆ S, (f a)*(g a sâ‚‚) := 
                 Finset.sum_product A S fun x â†¦ f x.1 * g x.1 x.2 --by apply Finset.sum_product 
          _ = âˆ‘ a âˆˆ A, (f a) * (âˆ‘ sâ‚‚ âˆˆ S, (g a sâ‚‚)) := by simp [Finset.mul_sum]  --Finset.sum_congr
          _ = âˆ‘ a âˆˆ A, (f a) * 1 := 
                Finset.sum_congr rfl fun x a â†¦ congrArg (fun y => (f x)*y) (h1 x)
          _ = âˆ‘ a âˆˆ A, (f a) := by ring_nf
          _ = 1 := h2

lemma prob_prod_hist {H : Finset (Hist m)} {A : Finset Î±} {S : Finset Ïƒ} (t : Hist m â†’ â„â‰¥0) 
      (f : Hist m â†’ Î± â†’ â„â‰¥0) (g : Hist m â†’ Î± â†’ Ïƒ â†’ â„â‰¥0) 
                (h1 : âˆ€ h : Hist m, âˆ€ a : Î±, âˆ‘ s âˆˆ S, g h a s = 1) 
                (h2 : âˆ€ h : Hist m, âˆ‘ a âˆˆ A, f h a = 1): 
(âˆ‘ âŸ¨h,a,sâŸ© âˆˆ (H Ã—Ë¢ A Ã—Ë¢ S), (t h) * (f h a * g h a s) ) = (âˆ‘ h âˆˆ H, t h)  := 
          have innsum {h : Hist m} : (âˆ‘ sa âˆˆ (A Ã—Ë¢ S), (f h sa.1) * (g h sa.1 sa.2) ) = 1 := 
                      by exact prob_prod (f h) (g h) (h1 h) (h2 h)
          calc
            âˆ‘ âŸ¨h,a,sâŸ© âˆˆ (H Ã—Ë¢ A Ã—Ë¢ S), (t h) * (f h a * g h a s) = 
            âˆ‘ h âˆˆ H, (âˆ‘ sa âˆˆ (A Ã—Ë¢ S), (t h) * (f h sa.1 * g h sa.1 sa.2) ) := 
                  by apply Finset.sum_product 
            _ = âˆ‘ h âˆˆ H, (t h) * (âˆ‘ âŸ¨a,sâŸ© âˆˆ (A Ã—Ë¢ S), (f h a * g h a s) ) := 
                  by simp [Finset.mul_sum]
            _ = âˆ‘ h âˆˆ H, (t h) * 1 := Finset.sum_congr rfl fun x a â†¦ congrArg (HMul.hMul (t x)) innsum
            _ = âˆ‘ h âˆˆ H, (t h) := by ring_nf


lemma prob_prod_ha {H : Finset (Hist m)} {Ï€ : Policy m} [DecidableEq Ïƒ] [Inhabited (Hist m)]: 
    âˆ‘ has âˆˆ (H Ã—Ë¢ m.A Ã—Ë¢ m.S), (probability_has Ï€ has) = âˆ‘ h âˆˆ H, probability Ï€ h :=
      prob_prod_hist (m:=m) (probability Ï€) (fun h a => Ï€.Ï€ h a) (fun h a s => m.P h.last a s)
      (by simp [m.prob]) Ï€.prob
    
/-- 
Show that history probabilities are actually a conditional probability 
distributions
-/
theorem probability_dist [Inhabited (Hist m)] [DecidableEq Ïƒ] (pre : Hist m) (Ï€ : Policy m) (T : â„•) : 
            (âˆ‘ h âˆˆ PHist pre T, probability Ï€ h) = (probability Ï€ pre) := 
      match T with
        --base case
        | Nat.zero => Finset.sum_singleton (probability Ï€) pre
        -- inductive case
        | Nat.succ t =>
              -- h1 is the inductive assumption
              have h1 : (âˆ‘ h âˆˆ PHist pre t, probability Ï€ h) = (probability Ï€ pre) := 
                         by apply probability_dist
              let HAS := ((PHist pre t) Ã—Ë¢ m.A Ã—Ë¢ m.S).map emb_tuple2hist
              calc
                âˆ‘ h âˆˆ PHist pre t.succ, probability Ï€ h = 
                  âˆ‘ h âˆˆ HAS, probability Ï€ h := rfl
                _ = âˆ‘ has âˆˆ ((PHist pre t) Ã—Ë¢ m.A Ã—Ë¢ m.S), (probability Ï€) (emb_tuple2hist has) :=
                      by apply Finset.sum_map
                _ = âˆ‘ has âˆˆ ((PHist pre t) Ã—Ë¢ m.A Ã—Ë¢ m.S), (probability_has Ï€ has) := 
                        by simp [hist_prob]
                _ = âˆ‘ h âˆˆ (PHist pre t), probability Ï€ h := by apply prob_prod_ha
                _ = probability Ï€ pre := by apply h1
              
                

example : m.A Ã—Ë¢ m.S = Finset.product m.A m.S := rfl
example {h : Hist m} : PHist h 0 = {h} := rfl
example {hâ‚€ : Hist m} {Ï€ : Policy m} [DecidableEq Ïƒ]: 
  (âˆ‘ h âˆˆ {hâ‚€}, probability Ï€ h) = (probability Ï€ hâ‚€) := by simp
  
/-

TODO:

1. Dynamic program for histories
2. Show that is the policy is Markov then also the value function is Markov

-/
