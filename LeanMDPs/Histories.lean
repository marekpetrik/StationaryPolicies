import Mathlib.Data.Nat.Defs
import Mathlib.Data.Real.Basic 
import Mathlib.Data.NNReal.Basic

--import Mathlib.Data.Set.Basic
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Finset.Image
import Mathlib.Logic.Function.Defs -- Injective

import Mathlib.Probability.ProbabilityMassFunction.Basic
--variable (Œ± œÉ : Type)

/-
In this file we define histories and operations that are related to them. 

* Defines an MDP
* Defines a history, which is a sequence of states and actions
* Defines a histories consistent with a partial sequence of states and actions
* A general randomized history-dependent policy
* The reward and probability of the history, which is used to compute the value function
* Value function for a history as the expected reward
-/


variable {œÉ Œ± : Type}
variable [Inhabited œÉ] [Inhabited Œ±]

open NNReal -- for ‚Ñù‚â•0 notation

/--
The Markov decision process definition 

TODO: Consider defining P and r only of the subtypes constructed from 
the Finsets ùíÆ and ùíú
-/
structure MDP (œÉ Œ± : Type) : Type where
  /-- states , TODO: consider \McS but causes issues-/
  SS : Finset œÉ
  /-- actions, TODO: consider \McA but causes issues  -/
  AA : Finset Œ±
  /-- transition probability s, a, s' -/
  -- TODO: should be a probability distribution
  P : œÉ ‚Üí Œ± ‚Üí œÉ ‚Üí ‚Ñù‚â•0
  /-- proof of transition probability --/
  prob : (s : œÉ) ‚Üí (a : Œ±) ‚Üí (‚àë s' ‚àà SS, P s a s') = 1
  /-- reward function s, a, s' -/
  r : œÉ ‚Üí Œ± ‚Üí œÉ ‚Üí ‚Ñù
  /-- initial state -/
  s‚ÇÄ : œÉ
  s‚ÇÄinSS : (s‚ÇÄ ‚àà SS)
  -- TODO: all these functions need to be only defined for states and actions
  -- should be using a Subtype {s // s ‚àà states} and Finset attach?

variable {m : MDP œÉ Œ±}

/--
Represents a history. The state is type œÉ and action is type Œ±.

The representation of the history is backwards to facilitate the 
application of a policy
-/
inductive Hist {œÉ Œ± : Type} (m : MDP œÉ Œ±)  : Type where
  | init : œÉ ‚Üí Hist m
  | prev : Hist m ‚Üí Œ± ‚Üí œÉ ‚Üí Hist m

/--
The length of the history corresponds to the zero-based step of the decision
-/
def Hist.length : Hist m ‚Üí ‚Ñï
  | init _ => 0
  | prev h _ _ => HAdd.hAdd (length h) 1

/-- returns the last state of the history -/
def Hist.laststate  : Hist m ‚Üí œÉ
  | init s => s
  | prev _ _ s => s

/-- appends the state and action to the history --/
def Hist.append (h : Hist m) (as : Œ± √ó œÉ) : Hist m :=
  Hist.prev h as.fst as.snd
  
/--
Creates new histories from combinations of shorter histories
and states and actions.
-/
def append_hist : Hist m √ó Œ± √ó œÉ ‚Üí Hist m
  | ‚ü®h, as‚ü© => h.append as

def append_hist_linv : Hist m ‚Üí Hist m √ó Œ± √ó œÉ
  | Hist.prev h a s => ‚ü® h, a, s ‚ü©
  -- the second case is not used
  | Hist.init _ => ‚ü® (Hist.init default), default, default ‚ü©

/-- Proves that history append has a left inverse. This is used 
    to show that the append_hist is an embedding, useful when 
    constructing a set of histories -/
lemma linv_append_hist : Function.LeftInverse (append_hist_linv (m := m) ) append_hist := fun _ => rfl

/--
checks if pre is the prefix of h. This is needed when defining value functions
-/
def isprefix (pre : Hist m) (h : Hist m) : Prop :=
  match pre, h with
    | Hist.init s‚ÇÅ, Hist.init s‚ÇÇ => s‚ÇÅ = s‚ÇÇ
    | Hist.init _, Hist.prev hp _ _ => isprefix pre hp 
    | Hist.prev _ _ _, Hist.init _ => False
    | Hist.prev h‚ÇÅ a‚ÇÅ s‚ÇÅ', Hist.prev  h‚ÇÇ a‚ÇÇ s‚ÇÇ' => 
        if h‚ÇÅ.length > h‚ÇÇ.length then
            False
        else if h‚ÇÅ.length < h‚ÇÇ.length then
            isprefix pre h‚ÇÇ
        else
            (a‚ÇÅ = a‚ÇÇ) ‚àß (s‚ÇÅ' = s‚ÇÇ') ‚àß (isprefix h‚ÇÅ h‚ÇÇ)


/--
A general randomized history-dependent policy
-/
structure Policy (m : MDP œÉ Œ±) : Type where
  œÄ : Hist m ‚Üí Œ± ‚Üí ‚Ñù‚â•0
  /-- proof that it sums to 1 for all states -/
  prob : (h : Hist m) ‚Üí (‚àë a ‚àà m.AA, œÄ h a) = 1

/- The set of all histories of length T -/
--def HistAll (T : ‚Ñï) := { h : Hist m | h.length = T }


-- Need to prove the function used in the construction is injective

 
--lemma append_hist_inj : Function.Injective (append_hist (œÉ := œÉ) (Œ± := Œ±)) :=
--   fun a‚ÇÅ a‚ÇÇ eq => Eq.rec (fun a b => rfl) eq
     

/--
Creates new histories from combinations of shorter histories
and states and actions.

The embedding guarantees it is injective
-/
def emb_hist_as : Hist m √ó Œ± √ó œÉ ‚Ü™ Hist m :=
 { toFun := append_hist, inj' := Function.LeftInverse.injective linv_append_hist }

/-- 
Set of all policies that follow history pre.
Note that this is just a definition of the set and not a specific instance of the set

The function allhist constructs all histories that satisfy the condition of this set

T is the number of steps beyond the history pre
-/
def PHist (pre : Hist m) (T : ‚Ñï) : Finset (Hist m) := 
    match T with 
      | Nat.zero => {pre}
      | Nat.succ t =>
        let HAS := Finset.product (PHist pre t) (Finset.product m.AA m.SS)
        Finset.map emb_hist_as HAS 
               
/--
Computes the probability of a history
-/
noncomputable def probability [DecidableEq œÉ] (œÄ : Policy m) : Hist m ‚Üí ‚Ñù‚â•0 
      | Hist.init s => if m.s‚ÇÄ = s then 1. else 0.
      | Hist.prev hp a s' => (m.P hp.laststate a s') * (œÄ.œÄ hp a) * (probability œÄ hp)  

/--
Computes the reward of a history
-/
noncomputable def reward : Hist m ‚Üí ‚Ñù 
    | Hist.init _ => 0.
    | Hist.prev hp a s'  =>  (m.r hp.laststate a s') + (reward hp)  


example {h : Hist m} : PHist h 0 = {h} := rfl
example {h‚ÇÄ : Hist m} {œÄ : Policy m} [DecidableEq œÉ]: 
  (‚àë h ‚àà {h‚ÇÄ}, probability œÄ h) = (probability œÄ h‚ÇÄ) := by simp
  

--example {S‚ÇÅ S‚ÇÇ : Finset œÉ} (s‚ÇÅ : œÉ) (f : ‚Ñù ) (g : œÉ ‚Üí ‚Ñù) : f*(‚àë s‚ÇÇ ‚àà S‚ÇÇ, (g s‚ÇÇ)) = ‚àë s‚ÇÇ ‚àà S‚ÇÇ, f*(g s‚ÇÇ) := by apply Finset.mul_sum


lemma prob_prod  {A : Finset Œ±} {S : Finset œÉ} (f : Œ± ‚Üí ‚Ñù) (g : œÉ ‚Üí ‚Ñù) (h1 : ‚àë s ‚àà S, g s = 1) (h2 : ‚àë a ‚àà A, f a = 1): 
          (‚àë sa ‚àà (A √óÀ¢ S), (f sa.1) * (g sa.2) ) = 1  := 
          calc 
          ‚àë sa ‚àà (A √óÀ¢ S), (f sa.1)*(g sa.2)  = ‚àë a ‚àà A, ‚àë s‚ÇÇ ‚àà S, (f a)*(g s‚ÇÇ) := by apply Finset.sum_product 
          _ = ‚àë a ‚àà A, (f a) * (‚àë s‚ÇÇ ‚àà S, (g s‚ÇÇ)) := by simp [Finset.mul_sum]  --Finset.sum_congr
          _ = ‚àë a ‚àà A, (f a) * 1 := by rw [h1]
          _ = ‚àë a ‚àà A, (f a)  := by ring_nf
          _ = 1 := by rw[h2]


example  {H : Finset (Hist m)} {A : Finset Œ±} {S : Finset œÉ} (t : Hist m ‚Üí ‚Ñù) (f : Œ± ‚Üí ‚Ñù) (g : œÉ ‚Üí ‚Ñù) (h1 : ‚àë s ‚àà S, g s = 1) (h2 : ‚àë a ‚àà A, f a = 1): 
          (‚àë has ‚àà (H √óÀ¢ A √óÀ¢ S), (t has.1) * (f has.2.1) * (g has.2.2) ) = (‚àë h ‚àà H, t h)  := 
          calc 
            ‚àë has ‚àà (H √óÀ¢ A √óÀ¢ S), (t has.1) * (f has.2.1) * (g has.2.2) = ‚àë h ‚àà H, (‚àë sa ‚àà (A √óÀ¢ S), (t h) * (f sa.1) * (g sa.2) ) := by apply Finset.sum_product 
            _ = ‚àë h ‚àà H, (t h) * (‚àë sa ‚àà (A √óÀ¢ S),  (f sa.1) * (g sa.2) ) := by simp [Finset.mul_sum]
            _ = ‚àë h ‚àà H, (t h) * 1 := by simp [prob_prod]
            _ = ‚àë h ‚àà H, (t h) := by ring_nf


/-- show that history probabilities are actually a conditional probability 
distribution 
-/
theorem probability_dist [DecidableEq œÉ] (pre : Hist m) (œÄ : Policy m) (T : ‚Ñï) : 
            (‚àë h ‚àà PHist pre T, probability œÄ h) = (probability œÄ pre) := 
      match T with
        | Nat.zero =>   -- TODO: simplify, see? Finset.sum_eq_single, apply?
              --have h1 : PHist pre 0 = {pre} := rfl
              show (‚àë h ‚àà {pre}, probability œÄ h) = (probability œÄ pre) by simp
              --by refine Finset.sum_eq_single
        | Nat.succ t =>
              have h1 : (‚àë h ‚àà PHist pre t, probability œÄ h) = (probability œÄ pre) := 
                         by apply probability_dist
              let HAS := Finset.product (PHist pre t) (Finset.product m.AA m.SS) 
              sorry

#check Finset.sum
#check Finset.univ


variable {S : Finset œÉ}

#check S √óÀ¢ S

/-

TODO:

1. Dynamic program for histories
2. Show that is the policy is Markov then also the value function is Markov
3. Show that histories are the PMF

-/
